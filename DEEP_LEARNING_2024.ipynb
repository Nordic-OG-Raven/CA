{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nordic-OG-Raven/CA/blob/main/DEEP_LEARNING_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b11944",
      "metadata": {
        "id": "40b11944"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e69df2a",
      "metadata": {
        "id": "4e69df2a"
      },
      "source": [
        "# The mathematical building blocks of neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f151007f",
      "metadata": {
        "id": "f151007f"
      },
      "source": [
        "## A first look at a neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c660ca5c",
      "metadata": {
        "id": "c660ca5c"
      },
      "source": [
        "**Loading the MNIST dataset in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b8672b",
      "metadata": {
        "id": "04b8672b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47ee63a3",
      "metadata": {
        "id": "47ee63a3"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29413501",
      "metadata": {
        "id": "29413501"
      },
      "outputs": [],
      "source": [
        "len(train_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "381640d1",
      "metadata": {
        "id": "381640d1"
      },
      "outputs": [],
      "source": [
        "train_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90595c21",
      "metadata": {
        "id": "90595c21"
      },
      "outputs": [],
      "source": [
        "test_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c75cd33b",
      "metadata": {
        "id": "c75cd33b"
      },
      "outputs": [],
      "source": [
        "len(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9215f6d",
      "metadata": {
        "id": "c9215f6d"
      },
      "outputs": [],
      "source": [
        "test_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf9e9bff",
      "metadata": {
        "id": "cf9e9bff"
      },
      "source": [
        "**The network architecture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dd66217",
      "metadata": {
        "id": "1dd66217"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "325f3e01",
      "metadata": {
        "id": "325f3e01"
      },
      "source": [
        "**The compilation step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c708239",
      "metadata": {
        "id": "3c708239"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e1f9050",
      "metadata": {
        "id": "4e1f9050"
      },
      "source": [
        "**Preparing the image data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47687a9a",
      "metadata": {
        "id": "47687a9a"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d3cdef4",
      "metadata": {
        "id": "4d3cdef4"
      },
      "source": [
        "**\"Fitting\" the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eda8a0a",
      "metadata": {
        "id": "8eda8a0a"
      },
      "outputs": [],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c830f85f",
      "metadata": {
        "id": "c830f85f"
      },
      "source": [
        "**Using the model to make predictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "485e5e67",
      "metadata": {
        "id": "485e5e67"
      },
      "outputs": [],
      "source": [
        "test_digits = test_images[0:10]\n",
        "predictions = model.predict(test_digits)\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed16fb5",
      "metadata": {
        "id": "aed16fb5"
      },
      "outputs": [],
      "source": [
        "predictions[0].argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fec8ae1",
      "metadata": {
        "id": "3fec8ae1"
      },
      "outputs": [],
      "source": [
        "predictions[0][7]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9fe61b50",
      "metadata": {
        "id": "9fe61b50"
      },
      "outputs": [],
      "source": [
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbba8bb9",
      "metadata": {
        "id": "fbba8bb9"
      },
      "source": [
        "**Evaluating the model on new data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "384b6a01",
      "metadata": {
        "id": "384b6a01"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"test_acc: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbefce8e",
      "metadata": {
        "id": "dbefce8e"
      },
      "source": [
        "## Data representations for neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef422cc",
      "metadata": {
        "id": "9ef422cc"
      },
      "source": [
        "### Scalars (rank-0 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfdc558d",
      "metadata": {
        "id": "bfdc558d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = np.array(12)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af2a4e25",
      "metadata": {
        "id": "af2a4e25"
      },
      "outputs": [],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a463c53a",
      "metadata": {
        "id": "a463c53a"
      },
      "source": [
        "### Vectors (rank-1 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f3d15c",
      "metadata": {
        "id": "65f3d15c"
      },
      "outputs": [],
      "source": [
        "x = np.array([12, 3, 6, 14, 7])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82192aac",
      "metadata": {
        "id": "82192aac"
      },
      "outputs": [],
      "source": [
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33c9482a",
      "metadata": {
        "id": "33c9482a"
      },
      "source": [
        "### Matrices (rank-2 tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d0f5ae2",
      "metadata": {
        "id": "9d0f5ae2"
      },
      "outputs": [],
      "source": [
        "x = np.array([[5, 78, 2, 34, 0],\n",
        "              [6, 79, 3, 35, 1],\n",
        "              [7, 80, 4, 36, 2]])\n",
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fd39fa1",
      "metadata": {
        "id": "7fd39fa1"
      },
      "source": [
        "### Rank-3 and higher-rank tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a0dd86",
      "metadata": {
        "id": "04a0dd86"
      },
      "outputs": [],
      "source": [
        "x = np.array([[[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]],\n",
        "              [[5, 78, 2, 34, 0],\n",
        "               [6, 79, 3, 35, 1],\n",
        "               [7, 80, 4, 36, 2]]])\n",
        "x.ndim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d6395ba",
      "metadata": {
        "id": "5d6395ba"
      },
      "source": [
        "### Key attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "345732b8",
      "metadata": {
        "id": "345732b8"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d7c0a5d",
      "metadata": {
        "id": "2d7c0a5d"
      },
      "outputs": [],
      "source": [
        "train_images.ndim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf34b1b",
      "metadata": {
        "id": "faf34b1b"
      },
      "outputs": [],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4404b38c",
      "metadata": {
        "id": "4404b38c"
      },
      "outputs": [],
      "source": [
        "train_images.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849ca4cd",
      "metadata": {
        "id": "849ca4cd"
      },
      "source": [
        "**Displaying the fourth digit**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73473af2",
      "metadata": {
        "id": "73473af2"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "digit = train_images[4]\n",
        "plt.imshow(digit, cmap=plt.cm.binary)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b114773",
      "metadata": {
        "id": "2b114773"
      },
      "outputs": [],
      "source": [
        "train_labels[4]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f378d422",
      "metadata": {
        "id": "f378d422"
      },
      "source": [
        "### Manipulating tensors in NumPy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "514159b2",
      "metadata": {
        "id": "514159b2"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[10:100]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca17141e",
      "metadata": {
        "id": "ca17141e"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[10:100, :, :]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4130c953",
      "metadata": {
        "id": "4130c953"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[10:100, 0:28, 0:28]\n",
        "my_slice.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55db318e",
      "metadata": {
        "id": "55db318e"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[:, 14:, 14:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "763501bb",
      "metadata": {
        "id": "763501bb"
      },
      "outputs": [],
      "source": [
        "my_slice = train_images[:, 7:-7, 7:-7]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d10fd936",
      "metadata": {
        "id": "d10fd936"
      },
      "source": [
        "### The notion of data batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dae79f1",
      "metadata": {
        "id": "6dae79f1"
      },
      "outputs": [],
      "source": [
        "batch = train_images[:128]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaa24a9f",
      "metadata": {
        "id": "eaa24a9f"
      },
      "outputs": [],
      "source": [
        "batch = train_images[128:256]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a1d640e",
      "metadata": {
        "id": "7a1d640e"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "batch = train_images[128 * n:128 * (n + 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6776b97",
      "metadata": {
        "id": "a6776b97"
      },
      "source": [
        "### Real-world examples of data tensors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6414f618",
      "metadata": {
        "id": "6414f618"
      },
      "source": [
        "### Vector data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbc23323",
      "metadata": {
        "id": "dbc23323"
      },
      "source": [
        "### Timeseries data or sequence data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e5680bc",
      "metadata": {
        "id": "1e5680bc"
      },
      "source": [
        "### Image data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "313371d3",
      "metadata": {
        "id": "313371d3"
      },
      "source": [
        "### Video data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79a08188",
      "metadata": {
        "id": "79a08188"
      },
      "source": [
        "## The gears of neural networks: tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0ea585",
      "metadata": {
        "id": "dc0ea585"
      },
      "source": [
        "### Element-wise operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f10c3d3",
      "metadata": {
        "id": "3f10c3d3"
      },
      "outputs": [],
      "source": [
        "def naive_relu(x):\n",
        "    assert len(x.shape) == 2\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] = max(x[i, j], 0)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4651c4b",
      "metadata": {
        "id": "b4651c4b"
      },
      "outputs": [],
      "source": [
        "def naive_add(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert x.shape == y.shape\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[i, j]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afba0a34",
      "metadata": {
        "id": "afba0a34"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "x = np.random.random((20, 100))\n",
        "y = np.random.random((20, 100))\n",
        "\n",
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = x + y\n",
        "    z = np.maximum(z, 0.)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a83fcdac",
      "metadata": {
        "id": "a83fcdac"
      },
      "outputs": [],
      "source": [
        "t0 = time.time()\n",
        "for _ in range(1000):\n",
        "    z = naive_add(x, y)\n",
        "    z = naive_relu(z)\n",
        "print(\"Took: {0:.2f} s\".format(time.time() - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94778141",
      "metadata": {
        "id": "94778141"
      },
      "source": [
        "### Broadcasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0187dc03",
      "metadata": {
        "id": "0187dc03"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "X = np.random.random((32, 10))\n",
        "y = np.random.random((10,))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed86958f",
      "metadata": {
        "id": "ed86958f"
      },
      "outputs": [],
      "source": [
        "y = np.expand_dims(y, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "424d6eab",
      "metadata": {
        "id": "424d6eab"
      },
      "outputs": [],
      "source": [
        "Y = np.concatenate([y] * 32, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ad06411",
      "metadata": {
        "id": "1ad06411"
      },
      "outputs": [],
      "source": [
        "def naive_add_matrix_and_vector(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    x = x.copy()\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            x[i, j] += y[j]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a62a73d9",
      "metadata": {
        "id": "a62a73d9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = np.random.random((64, 3, 32, 10))\n",
        "y = np.random.random((32, 10))\n",
        "z = np.maximum(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08d566e",
      "metadata": {
        "id": "c08d566e"
      },
      "source": [
        "### Tensor product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5977a0b9",
      "metadata": {
        "id": "5977a0b9"
      },
      "outputs": [],
      "source": [
        "x = np.random.random((32,))\n",
        "y = np.random.random((32,))\n",
        "z = np.dot(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc5e3519",
      "metadata": {
        "id": "bc5e3519"
      },
      "outputs": [],
      "source": [
        "def naive_vector_dot(x, y):\n",
        "    assert len(x.shape) == 1\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[0] == y.shape[0]\n",
        "    z = 0.\n",
        "    for i in range(x.shape[0]):\n",
        "        z += x[i] * y[i]\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7aceeae",
      "metadata": {
        "id": "a7aceeae"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 1\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(x.shape[1]):\n",
        "            z[i] += x[i, j] * y[j]\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecd672f3",
      "metadata": {
        "id": "ecd672f3"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_vector_dot(x, y):\n",
        "    z = np.zeros(x.shape[0])\n",
        "    for i in range(x.shape[0]):\n",
        "        z[i] = naive_vector_dot(x[i, :], y)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "608b4962",
      "metadata": {
        "id": "608b4962"
      },
      "outputs": [],
      "source": [
        "def naive_matrix_dot(x, y):\n",
        "    assert len(x.shape) == 2\n",
        "    assert len(y.shape) == 2\n",
        "    assert x.shape[1] == y.shape[0]\n",
        "    z = np.zeros((x.shape[0], y.shape[1]))\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(y.shape[1]):\n",
        "            row_x = x[i, :]\n",
        "            column_y = y[:, j]\n",
        "            z[i, j] = naive_vector_dot(row_x, column_y)\n",
        "    return z"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "402f9ece",
      "metadata": {
        "id": "402f9ece"
      },
      "source": [
        "### Tensor reshaping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca3d0069",
      "metadata": {
        "id": "ca3d0069"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28 * 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98e9ae6e",
      "metadata": {
        "id": "98e9ae6e"
      },
      "outputs": [],
      "source": [
        "x = np.array([[0., 1.],\n",
        "             [2., 3.],\n",
        "             [4., 5.]])\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7076833",
      "metadata": {
        "id": "a7076833"
      },
      "outputs": [],
      "source": [
        "x = x.reshape((6, 1))\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65471a0b",
      "metadata": {
        "id": "65471a0b"
      },
      "outputs": [],
      "source": [
        "x = np.zeros((300, 20))\n",
        "x = np.transpose(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "601cbd8e",
      "metadata": {
        "id": "601cbd8e"
      },
      "source": [
        "### Geometric interpretation of tensor operations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51653da1",
      "metadata": {
        "id": "51653da1"
      },
      "source": [
        "### A geometric interpretation of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e9dd67a",
      "metadata": {
        "id": "2e9dd67a"
      },
      "source": [
        "## The engine of neural networks: gradient-based optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75da4929",
      "metadata": {
        "id": "75da4929"
      },
      "source": [
        "### What's a derivative?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d75c618c",
      "metadata": {
        "id": "d75c618c"
      },
      "source": [
        "### Derivative of a tensor operation: the gradient"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea94bf1f",
      "metadata": {
        "id": "ea94bf1f"
      },
      "source": [
        "### Stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a79d796",
      "metadata": {
        "id": "0a79d796"
      },
      "source": [
        "### Chaining derivatives: The Backpropagation algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88653687",
      "metadata": {
        "id": "88653687"
      },
      "source": [
        "#### The chain rule"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9c2ae9e",
      "metadata": {
        "id": "e9c2ae9e"
      },
      "source": [
        "#### Automatic differentiation with computation graphs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce1014ba",
      "metadata": {
        "id": "ce1014ba"
      },
      "source": [
        "#### The gradient tape in TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b2a8b69",
      "metadata": {
        "id": "8b2a8b69"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "x = tf.Variable(0.)\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fab4b4ad",
      "metadata": {
        "id": "fab4b4ad"
      },
      "outputs": [],
      "source": [
        "x = tf.Variable(tf.random.uniform((2, 2)))\n",
        "with tf.GradientTape() as tape:\n",
        "    y = 2 * x + 3\n",
        "grad_of_y_wrt_x = tape.gradient(y, x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f83d393b",
      "metadata": {
        "id": "f83d393b"
      },
      "outputs": [],
      "source": [
        "W = tf.Variable(tf.random.uniform((2, 2)))\n",
        "b = tf.Variable(tf.zeros((2,)))\n",
        "x = tf.random.uniform((2, 2))\n",
        "with tf.GradientTape() as tape:\n",
        "    y = tf.matmul(x, W) + b\n",
        "grad_of_y_wrt_W_and_b = tape.gradient(y, [W, b])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5ac1e75",
      "metadata": {
        "id": "d5ac1e75"
      },
      "source": [
        "## Looking back at our first example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da34efa3",
      "metadata": {
        "id": "da34efa3"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78920a14",
      "metadata": {
        "id": "78920a14"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1718f2b8",
      "metadata": {
        "id": "1718f2b8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc8b0a7a",
      "metadata": {
        "id": "dc8b0a7a"
      },
      "outputs": [],
      "source": [
        "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feb006a4",
      "metadata": {
        "id": "feb006a4"
      },
      "source": [
        "### Reimplementing our first example from scratch in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aac96256",
      "metadata": {
        "id": "aac96256"
      },
      "source": [
        "#### A simple Dense class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3344699d",
      "metadata": {
        "id": "3344699d"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class NaiveDense:\n",
        "    def __init__(self, input_size, output_size, activation):\n",
        "        self.activation = activation\n",
        "\n",
        "        w_shape = (input_size, output_size)\n",
        "        w_initial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n",
        "        self.W = tf.Variable(w_initial_value)\n",
        "\n",
        "        b_shape = (output_size,)\n",
        "        b_initial_value = tf.zeros(b_shape)\n",
        "        self.b = tf.Variable(b_initial_value)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.activation(tf.matmul(inputs, self.W) + self.b)\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "        return [self.W, self.b]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d8f800",
      "metadata": {
        "id": "35d8f800"
      },
      "source": [
        "#### A simple Sequential class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "341a9632",
      "metadata": {
        "id": "341a9632"
      },
      "outputs": [],
      "source": [
        "class NaiveSequential:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "           x = layer(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def weights(self):\n",
        "       weights = []\n",
        "       for layer in self.layers:\n",
        "           weights += layer.weights\n",
        "       return weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54fe4b2d",
      "metadata": {
        "id": "54fe4b2d"
      },
      "outputs": [],
      "source": [
        "model = NaiveSequential([\n",
        "    NaiveDense(input_size=28 * 28, output_size=512, activation=tf.nn.relu),\n",
        "    NaiveDense(input_size=512, output_size=10, activation=tf.nn.softmax)\n",
        "])\n",
        "assert len(model.weights) == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac20aadd",
      "metadata": {
        "id": "ac20aadd"
      },
      "source": [
        "#### A batch generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a7bd4af",
      "metadata": {
        "id": "2a7bd4af"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class BatchGenerator:\n",
        "    def __init__(self, images, labels, batch_size=128):\n",
        "        assert len(images) == len(labels)\n",
        "        self.index = 0\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.batch_size = batch_size\n",
        "        self.num_batches = math.ceil(len(images) / batch_size)\n",
        "\n",
        "    def next(self):\n",
        "        images = self.images[self.index : self.index + self.batch_size]\n",
        "        labels = self.labels[self.index : self.index + self.batch_size]\n",
        "        self.index += self.batch_size\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48831f5f",
      "metadata": {
        "id": "48831f5f"
      },
      "source": [
        "### Running one training step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6504bd1",
      "metadata": {
        "id": "c6504bd1"
      },
      "outputs": [],
      "source": [
        "def one_training_step(model, images_batch, labels_batch):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images_batch)\n",
        "        per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            labels_batch, predictions)\n",
        "        average_loss = tf.reduce_mean(per_sample_losses)\n",
        "    gradients = tape.gradient(average_loss, model.weights)\n",
        "    update_weights(gradients, model.weights)\n",
        "    return average_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "057790ff",
      "metadata": {
        "id": "057790ff"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "    for g, w in zip(gradients, weights):\n",
        "        w.assign_sub(g * learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21751462",
      "metadata": {
        "id": "21751462"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "optimizer = optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "def update_weights(gradients, weights):\n",
        "    optimizer.apply_gradients(zip(gradients, weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a8e603f",
      "metadata": {
        "id": "1a8e603f"
      },
      "source": [
        "### The full training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdfe4694",
      "metadata": {
        "id": "fdfe4694"
      },
      "outputs": [],
      "source": [
        "def fit(model, images, labels, epochs, batch_size=128):\n",
        "    for epoch_counter in range(epochs):\n",
        "        print(f\"Epoch {epoch_counter}\")\n",
        "        batch_generator = BatchGenerator(images, labels)\n",
        "        for batch_counter in range(batch_generator.num_batches):\n",
        "            images_batch, labels_batch = batch_generator.next()\n",
        "            loss = one_training_step(model, images_batch, labels_batch)\n",
        "            if batch_counter % 100 == 0:\n",
        "                print(f\"loss at batch {batch_counter}: {loss:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deae741b",
      "metadata": {
        "id": "deae741b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "\n",
        "fit(model, train_images, train_labels, epochs=10, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088eda0e",
      "metadata": {
        "id": "088eda0e"
      },
      "source": [
        "### Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eab6a03",
      "metadata": {
        "id": "7eab6a03"
      },
      "outputs": [],
      "source": [
        "predictions = model(test_images)\n",
        "predictions = predictions.numpy()\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "matches = predicted_labels == test_labels\n",
        "print(f\"accuracy: {matches.mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61bbf214",
      "metadata": {
        "id": "61bbf214"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d385eeb2",
      "metadata": {
        "id": "d385eeb2"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f87a67d0",
      "metadata": {
        "id": "f87a67d0"
      },
      "source": [
        "# Introduction to Keras and TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6de65e",
      "metadata": {
        "id": "1c6de65e"
      },
      "source": [
        "## What's TensorFlow?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde2578a",
      "metadata": {
        "id": "bde2578a"
      },
      "source": [
        "## What's Keras?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dabb4b3c",
      "metadata": {
        "id": "dabb4b3c"
      },
      "source": [
        "## Keras and TensorFlow: A brief history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0c60c88",
      "metadata": {
        "id": "f0c60c88"
      },
      "source": [
        "## Setting up a deep-learning workspace"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72b8411",
      "metadata": {
        "id": "f72b8411"
      },
      "source": [
        "### Jupyter notebooks: The preferred way to run deep-learning experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7772ac9a",
      "metadata": {
        "id": "7772ac9a"
      },
      "source": [
        "### Using Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d823f3e4",
      "metadata": {
        "id": "d823f3e4"
      },
      "source": [
        "#### First steps with Colaboratory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f19a507d",
      "metadata": {
        "id": "f19a507d"
      },
      "source": [
        "#### Installing packages with pip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0690b8cf",
      "metadata": {
        "id": "0690b8cf"
      },
      "source": [
        "#### Using the GPU runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7e4ffa8",
      "metadata": {
        "id": "b7e4ffa8"
      },
      "source": [
        "## First steps with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d489a19",
      "metadata": {
        "id": "3d489a19"
      },
      "source": [
        "#### Constant tensors and variables"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cab8eae0",
      "metadata": {
        "id": "cab8eae0"
      },
      "source": [
        "**All-ones or all-zeros tensors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a2134e",
      "metadata": {
        "id": "c3a2134e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "x = tf.ones(shape=(2, 1))\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b28e52",
      "metadata": {
        "id": "47b28e52"
      },
      "outputs": [],
      "source": [
        "x = tf.zeros(shape=(2, 1))\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0977daaa",
      "metadata": {
        "id": "0977daaa"
      },
      "source": [
        "**Random tensors**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2cdd36f",
      "metadata": {
        "id": "e2cdd36f"
      },
      "outputs": [],
      "source": [
        "x = tf.random.normal(shape=(3, 1), mean=0., stddev=1.)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c5bb37c",
      "metadata": {
        "id": "8c5bb37c"
      },
      "outputs": [],
      "source": [
        "x = tf.random.uniform(shape=(3, 1), minval=0., maxval=1.)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7804fff6",
      "metadata": {
        "id": "7804fff6"
      },
      "source": [
        "**NumPy arrays are assignable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b06d17a9",
      "metadata": {
        "id": "b06d17a9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "x = np.ones(shape=(2, 2))\n",
        "x[0, 0] = 0."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ba1f4a",
      "metadata": {
        "id": "87ba1f4a"
      },
      "source": [
        "**Creating a TensorFlow variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7fe2360",
      "metadata": {
        "id": "a7fe2360"
      },
      "outputs": [],
      "source": [
        "v = tf.Variable(initial_value=tf.random.normal(shape=(3, 1)))\n",
        "print(v)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b04024",
      "metadata": {
        "id": "44b04024"
      },
      "source": [
        "**Assigning a value to a TensorFlow variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8425bd70",
      "metadata": {
        "id": "8425bd70"
      },
      "outputs": [],
      "source": [
        "v.assign(tf.ones((3, 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69772d43",
      "metadata": {
        "id": "69772d43"
      },
      "source": [
        "**Assigning a value to a subset of a TensorFlow variable**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6901eb00",
      "metadata": {
        "id": "6901eb00"
      },
      "outputs": [],
      "source": [
        "v[0, 0].assign(3.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d5a82bc",
      "metadata": {
        "id": "8d5a82bc"
      },
      "source": [
        "**Using `assign_add`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db81cf9d",
      "metadata": {
        "id": "db81cf9d"
      },
      "outputs": [],
      "source": [
        "v.assign_add(tf.ones((3, 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6abde6c5",
      "metadata": {
        "id": "6abde6c5"
      },
      "source": [
        "#### Tensor operations: Doing math in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df758b4",
      "metadata": {
        "id": "6df758b4"
      },
      "source": [
        "**A few basic math operations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7970de7",
      "metadata": {
        "id": "c7970de7"
      },
      "outputs": [],
      "source": [
        "a = tf.ones((2, 2))\n",
        "b = tf.square(a)\n",
        "c = tf.sqrt(a)\n",
        "d = b + c\n",
        "e = tf.matmul(a, b)\n",
        "e *= d"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e51de6e",
      "metadata": {
        "id": "7e51de6e"
      },
      "source": [
        "#### A second look at the GradientTape API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c12ba4c",
      "metadata": {
        "id": "6c12ba4c"
      },
      "source": [
        "**Using the `GradientTape`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d3bd237",
      "metadata": {
        "id": "4d3bd237"
      },
      "outputs": [],
      "source": [
        "input_var = tf.Variable(initial_value=3.)\n",
        "with tf.GradientTape() as tape:\n",
        "   result = tf.square(input_var)\n",
        "gradient = tape.gradient(result, input_var)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97e2103e",
      "metadata": {
        "id": "97e2103e"
      },
      "source": [
        "**Using `GradientTape` with constant tensor inputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7d17a0",
      "metadata": {
        "id": "df7d17a0"
      },
      "outputs": [],
      "source": [
        "input_const = tf.constant(3.)\n",
        "with tf.GradientTape() as tape:\n",
        "   tape.watch(input_const)\n",
        "   result = tf.square(input_const)\n",
        "gradient = tape.gradient(result, input_const)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b210e841",
      "metadata": {
        "id": "b210e841"
      },
      "source": [
        "**Using nested gradient tapes to compute second-order gradients**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c614249a",
      "metadata": {
        "id": "c614249a"
      },
      "outputs": [],
      "source": [
        "time = tf.Variable(0.)\n",
        "with tf.GradientTape() as outer_tape:\n",
        "    with tf.GradientTape() as inner_tape:\n",
        "        position =  4.9 * time ** 2\n",
        "    speed = inner_tape.gradient(position, time)\n",
        "acceleration = outer_tape.gradient(speed, time)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "536ff81b",
      "metadata": {
        "id": "536ff81b"
      },
      "source": [
        "#### An end-to-end example: A linear classifier in pure TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb5f5fdd",
      "metadata": {
        "id": "cb5f5fdd"
      },
      "source": [
        "**Generating two classes of random points in a 2D plane**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd82100f",
      "metadata": {
        "id": "fd82100f"
      },
      "outputs": [],
      "source": [
        "num_samples_per_class = 1000\n",
        "negative_samples = np.random.multivariate_normal(\n",
        "    mean=[0, 3],\n",
        "    cov=[[1, 0.5],[0.5, 1]],\n",
        "    size=num_samples_per_class)\n",
        "positive_samples = np.random.multivariate_normal(\n",
        "    mean=[3, 0],\n",
        "    cov=[[1, 0.5],[0.5, 1]],\n",
        "    size=num_samples_per_class)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "023ba99c",
      "metadata": {
        "id": "023ba99c"
      },
      "source": [
        "**Stacking the two classes into an array with shape (2000, 2)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35cb790b",
      "metadata": {
        "id": "35cb790b"
      },
      "outputs": [],
      "source": [
        "inputs = np.vstack((negative_samples, positive_samples)).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9373974b",
      "metadata": {
        "id": "9373974b"
      },
      "source": [
        "**Generating the corresponding targets (0 and 1)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe86620",
      "metadata": {
        "id": "5fe86620"
      },
      "outputs": [],
      "source": [
        "targets = np.vstack((np.zeros((num_samples_per_class, 1), dtype=\"float32\"),\n",
        "                     np.ones((num_samples_per_class, 1), dtype=\"float32\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f00c9023",
      "metadata": {
        "id": "f00c9023"
      },
      "source": [
        "**Plotting the two point classes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b702d46",
      "metadata": {
        "id": "3b702d46"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(inputs[:, 0], inputs[:, 1], c=targets[:, 0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bab2bb6",
      "metadata": {
        "id": "1bab2bb6"
      },
      "source": [
        "**Creating the linear classifier variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21a315c2",
      "metadata": {
        "id": "21a315c2"
      },
      "outputs": [],
      "source": [
        "input_dim = 2\n",
        "output_dim = 1\n",
        "W = tf.Variable(initial_value=tf.random.uniform(shape=(input_dim, output_dim)))\n",
        "b = tf.Variable(initial_value=tf.zeros(shape=(output_dim,)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2034a5ba",
      "metadata": {
        "id": "2034a5ba"
      },
      "source": [
        "**The forward pass function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04d48464",
      "metadata": {
        "id": "04d48464"
      },
      "outputs": [],
      "source": [
        "def model(inputs):\n",
        "    return tf.matmul(inputs, W) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e9d94c4",
      "metadata": {
        "id": "1e9d94c4"
      },
      "source": [
        "**The mean squared error loss function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "400a9e93",
      "metadata": {
        "id": "400a9e93"
      },
      "outputs": [],
      "source": [
        "def square_loss(targets, predictions):\n",
        "    per_sample_losses = tf.square(targets - predictions)\n",
        "    return tf.reduce_mean(per_sample_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76829966",
      "metadata": {
        "id": "76829966"
      },
      "source": [
        "**The training step function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79ce1799",
      "metadata": {
        "id": "79ce1799"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "def training_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs)\n",
        "        loss = square_loss(targets, predictions)\n",
        "    grad_loss_wrt_W, grad_loss_wrt_b = tape.gradient(loss, [W, b])\n",
        "    W.assign_sub(grad_loss_wrt_W * learning_rate)\n",
        "    b.assign_sub(grad_loss_wrt_b * learning_rate)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0479fdf7",
      "metadata": {
        "id": "0479fdf7"
      },
      "source": [
        "**The batch training loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e3dc34",
      "metadata": {
        "id": "65e3dc34"
      },
      "outputs": [],
      "source": [
        "for step in range(40):\n",
        "    loss = training_step(inputs, targets)\n",
        "    print(f\"Loss at step {step}: {loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ad2e5e",
      "metadata": {
        "id": "58ad2e5e"
      },
      "outputs": [],
      "source": [
        "predictions = model(inputs)\n",
        "plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c25133bb",
      "metadata": {
        "id": "c25133bb"
      },
      "outputs": [],
      "source": [
        "x = np.linspace(-1, 4, 100)\n",
        "y = - W[0] /  W[1] * x + (0.5 - b) / W[1]\n",
        "plt.plot(x, y, \"-r\")\n",
        "plt.scatter(inputs[:, 0], inputs[:, 1], c=predictions[:, 0] > 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caf21f87",
      "metadata": {
        "id": "caf21f87"
      },
      "source": [
        "## Anatomy of a neural network: Understanding core Keras APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f2db250",
      "metadata": {
        "id": "4f2db250"
      },
      "source": [
        "### Layers: The building blocks of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92e153f6",
      "metadata": {
        "id": "92e153f6"
      },
      "source": [
        "#### The base Layer class in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "200e3b02",
      "metadata": {
        "id": "200e3b02"
      },
      "source": [
        "**A `Dense` layer implemented as a `Layer` subclass**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4cb3ac1",
      "metadata": {
        "id": "a4cb3ac1"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "class SimpleDense(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units, activation=None):\n",
        "        super().__init__()\n",
        "        self.units = units\n",
        "        self.activation = activation\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        input_dim = input_shape[-1]\n",
        "        self.W = self.add_weight(shape=(input_dim, self.units),\n",
        "                                 initializer=\"random_normal\")\n",
        "        self.b = self.add_weight(shape=(self.units,),\n",
        "                                 initializer=\"zeros\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        y = tf.matmul(inputs, self.W) + self.b\n",
        "        if self.activation is not None:\n",
        "            y = self.activation(y)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9f12654",
      "metadata": {
        "id": "b9f12654"
      },
      "outputs": [],
      "source": [
        "my_dense = SimpleDense(units=32, activation=tf.nn.relu)\n",
        "input_tensor = tf.ones(shape=(2, 784))\n",
        "output_tensor = my_dense(input_tensor)\n",
        "print(output_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83751f42",
      "metadata": {
        "id": "83751f42"
      },
      "source": [
        "#### Automatic shape inference: Building layers on the fly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae95aebe",
      "metadata": {
        "id": "ae95aebe"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "layer = layers.Dense(32, activation=\"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49c7e171",
      "metadata": {
        "id": "49c7e171"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "model = models.Sequential([\n",
        "    layers.Dense(32, activation=\"relu\"),\n",
        "    layers.Dense(32)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3ed7c43",
      "metadata": {
        "id": "f3ed7c43"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    SimpleDense(32, activation=\"relu\"),\n",
        "    SimpleDense(64, activation=\"relu\"),\n",
        "    SimpleDense(32, activation=\"relu\"),\n",
        "    SimpleDense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab2bc283",
      "metadata": {
        "id": "ab2bc283"
      },
      "source": [
        "### From layers to models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c148a02",
      "metadata": {
        "id": "5c148a02"
      },
      "source": [
        "### The \"compile\" step: Configuring the learning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44cd447e",
      "metadata": {
        "id": "44cd447e"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([keras.layers.Dense(1)])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"mean_squared_error\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd80d16c",
      "metadata": {
        "id": "cd80d16c"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        "              loss=keras.losses.MeanSquaredError(),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f231769b",
      "metadata": {
        "id": "f231769b"
      },
      "source": [
        "### Picking a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12920fb",
      "metadata": {
        "id": "d12920fb"
      },
      "source": [
        "### Understanding the fit() method"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84d9131d",
      "metadata": {
        "id": "84d9131d"
      },
      "source": [
        "**Calling `fit()` with NumPy data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "042900fc",
      "metadata": {
        "id": "042900fc"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    inputs,\n",
        "    targets,\n",
        "    epochs=5,\n",
        "    batch_size=128\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0830593",
      "metadata": {
        "id": "b0830593"
      },
      "outputs": [],
      "source": [
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acbe392f",
      "metadata": {
        "id": "acbe392f"
      },
      "source": [
        "### Monitoring loss and metrics on validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f727c19f",
      "metadata": {
        "id": "f727c19f"
      },
      "source": [
        "**Using the `validation_data` argument**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a7b96b",
      "metadata": {
        "id": "63a7b96b"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([keras.layers.Dense(1)])\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
        "              loss=keras.losses.MeanSquaredError(),\n",
        "              metrics=[keras.metrics.BinaryAccuracy()])\n",
        "\n",
        "indices_permutation = np.random.permutation(len(inputs))\n",
        "shuffled_inputs = inputs[indices_permutation]\n",
        "shuffled_targets = targets[indices_permutation]\n",
        "\n",
        "num_validation_samples = int(0.3 * len(inputs))\n",
        "val_inputs = shuffled_inputs[:num_validation_samples]\n",
        "val_targets = shuffled_targets[:num_validation_samples]\n",
        "training_inputs = shuffled_inputs[num_validation_samples:]\n",
        "training_targets = shuffled_targets[num_validation_samples:]\n",
        "model.fit(\n",
        "    training_inputs,\n",
        "    training_targets,\n",
        "    epochs=5,\n",
        "    batch_size=16,\n",
        "    validation_data=(val_inputs, val_targets)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "114366a6",
      "metadata": {
        "id": "114366a6"
      },
      "source": [
        "### Inference: Using a model after training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d86018ae",
      "metadata": {
        "id": "d86018ae"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(val_inputs, batch_size=128)\n",
        "print(predictions[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c208f20",
      "metadata": {
        "id": "8c208f20"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65c7fb83",
      "metadata": {
        "id": "65c7fb83"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41a7989f",
      "metadata": {
        "id": "41a7989f"
      },
      "source": [
        "# Getting started with neural networks: Classification and regression"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04efb3ad",
      "metadata": {
        "id": "04efb3ad"
      },
      "source": [
        "## Classifying movie reviews: A binary classification example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e6fcb2e",
      "metadata": {
        "id": "5e6fcb2e"
      },
      "source": [
        "### The IMDB dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4944309a",
      "metadata": {
        "id": "4944309a"
      },
      "source": [
        "**Loading the IMDB dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55919e51",
      "metadata": {
        "id": "55919e51"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n",
        "    num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6a8a87f",
      "metadata": {
        "id": "c6a8a87f"
      },
      "outputs": [],
      "source": [
        "train_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d90afcc9",
      "metadata": {
        "id": "d90afcc9"
      },
      "outputs": [],
      "source": [
        "train_labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d2e7dd",
      "metadata": {
        "id": "c0d2e7dd"
      },
      "outputs": [],
      "source": [
        "max([max(sequence) for sequence in train_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8bfb6a",
      "metadata": {
        "id": "1d8bfb6a"
      },
      "source": [
        "**Decoding reviews back to text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d068c50d",
      "metadata": {
        "id": "d068c50d"
      },
      "outputs": [],
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict(\n",
        "    [(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = \" \".join(\n",
        "    [reverse_word_index.get(i - 3, \"?\") for i in train_data[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12bc91e1",
      "metadata": {
        "id": "12bc91e1"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "428aac36",
      "metadata": {
        "id": "428aac36"
      },
      "source": [
        "**Encoding the integer sequences via multi-hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d563fa60",
      "metadata": {
        "id": "d563fa60"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        for j in sequence:\n",
        "            results[i, j] = 1.\n",
        "    return results\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618ddb87",
      "metadata": {
        "id": "618ddb87"
      },
      "outputs": [],
      "source": [
        "x_train[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9415989",
      "metadata": {
        "id": "e9415989"
      },
      "outputs": [],
      "source": [
        "y_train = np.asarray(train_labels).astype(\"float32\")\n",
        "y_test = np.asarray(test_labels).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90c933f3",
      "metadata": {
        "id": "90c933f3"
      },
      "source": [
        "### Building your model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70acdf97",
      "metadata": {
        "id": "70acdf97"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee779fe7",
      "metadata": {
        "id": "ee779fe7"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de24eb40",
      "metadata": {
        "id": "de24eb40"
      },
      "source": [
        "**Compiling the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65691b04",
      "metadata": {
        "id": "65691b04"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab34d5fd",
      "metadata": {
        "id": "ab34d5fd"
      },
      "source": [
        "### Validating your approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbfbfd41",
      "metadata": {
        "id": "cbfbfd41"
      },
      "source": [
        "**Setting aside a validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db32acd1",
      "metadata": {
        "id": "db32acd1"
      },
      "outputs": [],
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24be8fe7",
      "metadata": {
        "id": "24be8fe7"
      },
      "source": [
        "**Training your model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd3be7e3",
      "metadata": {
        "id": "dd3be7e3"
      },
      "outputs": [],
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67766f7a",
      "metadata": {
        "id": "67766f7a"
      },
      "outputs": [],
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa4228f6",
      "metadata": {
        "id": "aa4228f6"
      },
      "source": [
        "**Plotting the training and validation loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c9168f",
      "metadata": {
        "id": "08c9168f"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "history_dict = history.history\n",
        "loss_values = history_dict[\"loss\"]\n",
        "val_loss_values = history_dict[\"val_loss\"]\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d415d0ba",
      "metadata": {
        "id": "d415d0ba"
      },
      "source": [
        "**Plotting the training and validation accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950da9f7",
      "metadata": {
        "id": "950da9f7"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "acc = history_dict[\"accuracy\"]\n",
        "val_acc = history_dict[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation acc\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f0c6486",
      "metadata": {
        "id": "4f0c6486"
      },
      "source": [
        "**Retraining a model from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c895367",
      "metadata": {
        "id": "8c895367"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c6651b",
      "metadata": {
        "id": "a0c6651b"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0601ae80",
      "metadata": {
        "id": "0601ae80"
      },
      "source": [
        "### Using a trained model to generate predictions on new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c9e981d",
      "metadata": {
        "id": "1c9e981d"
      },
      "outputs": [],
      "source": [
        "model.predict(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b244f52e",
      "metadata": {
        "id": "b244f52e"
      },
      "source": [
        "### Further experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46087d80",
      "metadata": {
        "id": "46087d80"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62774705",
      "metadata": {
        "id": "62774705"
      },
      "source": [
        "## Classifying newswires: A multiclass classification example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a21863",
      "metadata": {
        "id": "d6a21863"
      },
      "source": [
        "### The Reuters dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16924332",
      "metadata": {
        "id": "16924332"
      },
      "source": [
        "**Loading the Reuters dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc230d1",
      "metadata": {
        "id": "efc230d1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import reuters\n",
        "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(\n",
        "    num_words=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2786e801",
      "metadata": {
        "id": "2786e801"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b103b60",
      "metadata": {
        "id": "7b103b60"
      },
      "outputs": [],
      "source": [
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed169f35",
      "metadata": {
        "id": "ed169f35"
      },
      "outputs": [],
      "source": [
        "train_data[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0f984f3",
      "metadata": {
        "id": "c0f984f3"
      },
      "source": [
        "**Decoding newswires back to text**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa3cf988",
      "metadata": {
        "id": "aa3cf988"
      },
      "outputs": [],
      "source": [
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_newswire = \" \".join([reverse_word_index.get(i - 3, \"?\") for i in\n",
        "    train_data[0]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebca4624",
      "metadata": {
        "id": "ebca4624"
      },
      "outputs": [],
      "source": [
        "train_labels[10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef21d2d8",
      "metadata": {
        "id": "ef21d2d8"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8642b7",
      "metadata": {
        "id": "af8642b7"
      },
      "source": [
        "**Encoding the input data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8f498ca",
      "metadata": {
        "id": "f8f498ca"
      },
      "outputs": [],
      "source": [
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c8eca12",
      "metadata": {
        "id": "7c8eca12"
      },
      "source": [
        "**Encoding the labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3283321c",
      "metadata": {
        "id": "3283321c"
      },
      "outputs": [],
      "source": [
        "def to_one_hot(labels, dimension=46):\n",
        "    results = np.zeros((len(labels), dimension))\n",
        "    for i, label in enumerate(labels):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "y_train = to_one_hot(train_labels)\n",
        "y_test = to_one_hot(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d876d968",
      "metadata": {
        "id": "d876d968"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "y_train = to_categorical(train_labels)\n",
        "y_test = to_categorical(test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc8f8d72",
      "metadata": {
        "id": "bc8f8d72"
      },
      "source": [
        "### Building your model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8685ff6",
      "metadata": {
        "id": "e8685ff6"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6c55fe4",
      "metadata": {
        "id": "f6c55fe4"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(46, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ee759d",
      "metadata": {
        "id": "47ee759d"
      },
      "source": [
        "**Compiling the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b1a19cb",
      "metadata": {
        "id": "2b1a19cb"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39bd6e99",
      "metadata": {
        "id": "39bd6e99"
      },
      "source": [
        "### Validating your approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e599080",
      "metadata": {
        "id": "2e599080"
      },
      "source": [
        "**Setting aside a validation set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e997dab6",
      "metadata": {
        "id": "e997dab6"
      },
      "outputs": [],
      "source": [
        "x_val = x_train[:1000]\n",
        "partial_x_train = x_train[1000:]\n",
        "y_val = y_train[:1000]\n",
        "partial_y_train = y_train[1000:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "024586c6",
      "metadata": {
        "id": "024586c6"
      },
      "source": [
        "**Training the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e4d252d",
      "metadata": {
        "id": "4e4d252d"
      },
      "outputs": [],
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8675758",
      "metadata": {
        "id": "f8675758"
      },
      "source": [
        "**Plotting the training and validation loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f9a78c",
      "metadata": {
        "id": "17f9a78c"
      },
      "outputs": [],
      "source": [
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "225d5e67",
      "metadata": {
        "id": "225d5e67"
      },
      "source": [
        "**Plotting the training and validation accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e83cd704",
      "metadata": {
        "id": "e83cd704"
      },
      "outputs": [],
      "source": [
        "plt.clf()\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "217bddcc",
      "metadata": {
        "id": "217bddcc"
      },
      "source": [
        "**Retraining a model from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04576a75",
      "metadata": {
        "id": "04576a75"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Dense(64, activation=\"relu\"),\n",
        "  layers.Dense(64, activation=\"relu\"),\n",
        "  layers.Dense(46, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(x_train,\n",
        "          y_train,\n",
        "          epochs=9,\n",
        "          batch_size=512)\n",
        "results = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef4279f1",
      "metadata": {
        "id": "ef4279f1"
      },
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f341fd1",
      "metadata": {
        "id": "8f341fd1"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "test_labels_copy = copy.copy(test_labels)\n",
        "np.random.shuffle(test_labels_copy)\n",
        "hits_array = np.array(test_labels) == np.array(test_labels_copy)\n",
        "hits_array.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "754e6cbe",
      "metadata": {
        "id": "754e6cbe"
      },
      "source": [
        "### Generating predictions on new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05cdb8cf",
      "metadata": {
        "id": "05cdb8cf"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90cc34b5",
      "metadata": {
        "id": "90cc34b5"
      },
      "outputs": [],
      "source": [
        "predictions[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b31ec023",
      "metadata": {
        "id": "b31ec023"
      },
      "outputs": [],
      "source": [
        "np.sum(predictions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cfc35a5",
      "metadata": {
        "id": "0cfc35a5"
      },
      "outputs": [],
      "source": [
        "np.argmax(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada1494f",
      "metadata": {
        "id": "ada1494f"
      },
      "source": [
        "### A different way to handle the labels and the loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9e5d608",
      "metadata": {
        "id": "a9e5d608"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(train_labels)\n",
        "y_test = np.array(test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73d32dfd",
      "metadata": {
        "id": "73d32dfd"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b167f6ae",
      "metadata": {
        "id": "b167f6ae"
      },
      "source": [
        "### The importance of having sufficiently large intermediate layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5dd5a04",
      "metadata": {
        "id": "e5dd5a04"
      },
      "source": [
        "**A model with an information bottleneck**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f103386e",
      "metadata": {
        "id": "f103386e"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(4, activation=\"relu\"),\n",
        "    layers.Dense(46, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(partial_x_train,\n",
        "          partial_y_train,\n",
        "          epochs=20,\n",
        "          batch_size=128,\n",
        "          validation_data=(x_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ca7b54e",
      "metadata": {
        "id": "2ca7b54e"
      },
      "source": [
        "### Further experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40054335",
      "metadata": {
        "id": "40054335"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45f08686",
      "metadata": {
        "id": "45f08686"
      },
      "source": [
        "## Predicting house prices: A regression example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3426d903",
      "metadata": {
        "id": "3426d903"
      },
      "source": [
        "### The Boston Housing Price dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f89128",
      "metadata": {
        "id": "f6f89128"
      },
      "source": [
        "**Loading the Boston housing dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32bf43c4",
      "metadata": {
        "id": "32bf43c4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import boston_housing\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3f8d58",
      "metadata": {
        "id": "bc3f8d58"
      },
      "outputs": [],
      "source": [
        "train_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "744e12dd",
      "metadata": {
        "id": "744e12dd"
      },
      "outputs": [],
      "source": [
        "test_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bddcd085",
      "metadata": {
        "id": "bddcd085"
      },
      "outputs": [],
      "source": [
        "train_targets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718cbcbe",
      "metadata": {
        "id": "718cbcbe"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44b674cb",
      "metadata": {
        "id": "44b674cb"
      },
      "source": [
        "**Normalizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4166c3d8",
      "metadata": {
        "id": "4166c3d8"
      },
      "outputs": [],
      "source": [
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "test_data -= mean\n",
        "test_data /= std"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23479cd9",
      "metadata": {
        "id": "23479cd9"
      },
      "source": [
        "### Building your model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40d9156e",
      "metadata": {
        "id": "40d9156e"
      },
      "source": [
        "**Model definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1306499a",
      "metadata": {
        "id": "1306499a"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a0ea36",
      "metadata": {
        "id": "b3a0ea36"
      },
      "source": [
        "### Validating your approach using K-fold validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d21de15d",
      "metadata": {
        "id": "d21de15d"
      },
      "source": [
        "**K-fold validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "620ec524",
      "metadata": {
        "id": "620ec524"
      },
      "outputs": [],
      "source": [
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "for i in range(k):\n",
        "    print(f\"Processing fold #{i}\")\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    model = build_model()\n",
        "    model.fit(partial_train_data, partial_train_targets,\n",
        "              epochs=num_epochs, batch_size=16, verbose=0)\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "    all_scores.append(val_mae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db58d11e",
      "metadata": {
        "id": "db58d11e"
      },
      "outputs": [],
      "source": [
        "all_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12ad884e",
      "metadata": {
        "id": "12ad884e"
      },
      "outputs": [],
      "source": [
        "np.mean(all_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aa22507",
      "metadata": {
        "id": "8aa22507"
      },
      "source": [
        "**Saving the validation logs at each fold**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211049c7",
      "metadata": {
        "id": "211049c7"
      },
      "outputs": [],
      "source": [
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "for i in range(k):\n",
        "    print(f\"Processing fold #{i}\")\n",
        "    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate(\n",
        "        [train_data[:i * num_val_samples],\n",
        "         train_data[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    partial_train_targets = np.concatenate(\n",
        "        [train_targets[:i * num_val_samples],\n",
        "         train_targets[(i + 1) * num_val_samples:]],\n",
        "        axis=0)\n",
        "    model = build_model()\n",
        "    history = model.fit(partial_train_data, partial_train_targets,\n",
        "                        validation_data=(val_data, val_targets),\n",
        "                        epochs=num_epochs, batch_size=16, verbose=0)\n",
        "    mae_history = history.history[\"val_mae\"]\n",
        "    all_mae_histories.append(mae_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fdb5f3",
      "metadata": {
        "id": "59fdb5f3"
      },
      "source": [
        "**Building the history of successive mean K-fold validation scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ed5faab",
      "metadata": {
        "id": "3ed5faab"
      },
      "outputs": [],
      "source": [
        "average_mae_history = [\n",
        "    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f289178b",
      "metadata": {
        "id": "f289178b"
      },
      "source": [
        "**Plotting validation scores**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ba0cf0",
      "metadata": {
        "id": "36ba0cf0"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation MAE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a752a4f7",
      "metadata": {
        "id": "a752a4f7"
      },
      "source": [
        "**Plotting validation scores, excluding the first 10 data points**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2903a24",
      "metadata": {
        "id": "d2903a24"
      },
      "outputs": [],
      "source": [
        "truncated_mae_history = average_mae_history[10:]\n",
        "plt.plot(range(1, len(truncated_mae_history) + 1), truncated_mae_history)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Validation MAE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004df5c5",
      "metadata": {
        "id": "004df5c5"
      },
      "source": [
        "**Training the final model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1444c3",
      "metadata": {
        "id": "1c1444c3"
      },
      "outputs": [],
      "source": [
        "model = build_model()\n",
        "model.fit(train_data, train_targets,\n",
        "          epochs=130, batch_size=16, verbose=0)\n",
        "test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba381f3b",
      "metadata": {
        "id": "ba381f3b"
      },
      "outputs": [],
      "source": [
        "test_mae_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c749abfb",
      "metadata": {
        "id": "c749abfb"
      },
      "source": [
        "### Generating predictions on new data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb04ec2b",
      "metadata": {
        "id": "fb04ec2b"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(test_data)\n",
        "predictions[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f177a17d",
      "metadata": {
        "id": "f177a17d"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556f3900",
      "metadata": {
        "id": "556f3900"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a24c90ea",
      "metadata": {
        "id": "a24c90ea"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22ea34d4",
      "metadata": {
        "id": "22ea34d4"
      },
      "source": [
        "# Fundamentals of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aded11af",
      "metadata": {
        "id": "aded11af"
      },
      "source": [
        "## Generalization: The goal of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc9d565",
      "metadata": {
        "id": "9bc9d565"
      },
      "source": [
        "### Underfitting and overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1567f6c0",
      "metadata": {
        "id": "1567f6c0"
      },
      "source": [
        "#### Noisy training data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1df028a2",
      "metadata": {
        "id": "1df028a2"
      },
      "source": [
        "#### Ambiguous features"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610aa786",
      "metadata": {
        "id": "610aa786"
      },
      "source": [
        "#### Rare features and spurious correlations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d37aa20",
      "metadata": {
        "id": "6d37aa20"
      },
      "source": [
        "**Adding white-noise channels or all-zeros channels to MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1660480",
      "metadata": {
        "id": "d1660480"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(train_images, train_labels), _ = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "train_images_with_noise_channels = np.concatenate(\n",
        "    [train_images, np.random.random((len(train_images), 784))], axis=1)\n",
        "\n",
        "train_images_with_zeros_channels = np.concatenate(\n",
        "    [train_images, np.zeros((len(train_images), 784))], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "468f320a",
      "metadata": {
        "id": "468f320a"
      },
      "source": [
        "**Training the same model on MNIST data with noise channels or all-zero channels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46b6f9ba",
      "metadata": {
        "id": "46b6f9ba"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(512, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"sparse_categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "history_noise = model.fit(\n",
        "    train_images_with_noise_channels, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)\n",
        "\n",
        "model = get_model()\n",
        "history_zeros = model.fit(\n",
        "    train_images_with_zeros_channels, train_labels,\n",
        "    epochs=10,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7156a9f2",
      "metadata": {
        "id": "7156a9f2"
      },
      "source": [
        "**Plotting a validation accuracy comparison**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "854ebacb",
      "metadata": {
        "id": "854ebacb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_acc_noise = history_noise.history[\"val_accuracy\"]\n",
        "val_acc_zeros = history_zeros.history[\"val_accuracy\"]\n",
        "epochs = range(1, 11)\n",
        "plt.plot(epochs, val_acc_noise, \"b-\",\n",
        "         label=\"Validation accuracy with noise channels\")\n",
        "plt.plot(epochs, val_acc_zeros, \"b--\",\n",
        "         label=\"Validation accuracy with zeros channels\")\n",
        "plt.title(\"Effect of noise channels on validation accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c0c53d",
      "metadata": {
        "id": "09c0c53d"
      },
      "source": [
        "### The nature of generalization in deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7fe60ac",
      "metadata": {
        "id": "a7fe60ac"
      },
      "source": [
        "**Fitting a MNIST model with randomly shuffled labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955ed05e",
      "metadata": {
        "id": "955ed05e"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), _ = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "random_train_labels = train_labels[:]\n",
        "np.random.shuffle(random_train_labels)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, random_train_labels,\n",
        "          epochs=100,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81639029",
      "metadata": {
        "id": "81639029"
      },
      "source": [
        "#### The manifold hypothesis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28ad3bf0",
      "metadata": {
        "id": "28ad3bf0"
      },
      "source": [
        "#### Interpolation as a source of generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eacd1dd4",
      "metadata": {
        "id": "eacd1dd4"
      },
      "source": [
        "#### Why deep learning works"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15098fbf",
      "metadata": {
        "id": "15098fbf"
      },
      "source": [
        "#### Training data is paramount"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11d029a8",
      "metadata": {
        "id": "11d029a8"
      },
      "source": [
        "## Evaluating machine-learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4947a196",
      "metadata": {
        "id": "4947a196"
      },
      "source": [
        "### Training, validation, and test sets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e724ab5e",
      "metadata": {
        "id": "e724ab5e"
      },
      "source": [
        "#### Simple hold-out validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b05e0d7b",
      "metadata": {
        "id": "b05e0d7b"
      },
      "source": [
        "#### K-fold validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df32c3fc",
      "metadata": {
        "id": "df32c3fc"
      },
      "source": [
        "#### Iterated K-fold validation with shuffling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb073fc5",
      "metadata": {
        "id": "cb073fc5"
      },
      "source": [
        "### Beating a common-sense baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ca78807",
      "metadata": {
        "id": "3ca78807"
      },
      "source": [
        "### Things to keep in mind about model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7edf28e2",
      "metadata": {
        "id": "7edf28e2"
      },
      "source": [
        "## Improving model fit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f5d202b",
      "metadata": {
        "id": "1f5d202b"
      },
      "source": [
        "### Tuning key gradient descent parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd07eb71",
      "metadata": {
        "id": "bd07eb71"
      },
      "source": [
        "**Training a MNIST model with an incorrectly high learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a2830dc",
      "metadata": {
        "id": "0a2830dc"
      },
      "outputs": [],
      "source": [
        "(train_images, train_labels), _ = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(1.),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "592e807a",
      "metadata": {
        "id": "592e807a"
      },
      "source": [
        "**The same model with a more appropriate learning rate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af3a450",
      "metadata": {
        "id": "2af3a450"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(1e-2),\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          batch_size=128,\n",
        "          validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d54b8813",
      "metadata": {
        "id": "d54b8813"
      },
      "source": [
        "### Leveraging better architecture priors"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f1d193",
      "metadata": {
        "id": "82f1d193"
      },
      "source": [
        "### Increasing model capacity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a83a04",
      "metadata": {
        "id": "38a83a04"
      },
      "source": [
        "**A simple logistic regression on MNIST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "782b04ec",
      "metadata": {
        "id": "782b04ec"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([layers.Dense(10, activation=\"softmax\")])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_small_model = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c34db6",
      "metadata": {
        "id": "68c34db6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "val_loss = history_small_model.history[\"val_loss\"]\n",
        "epochs = range(1, 21)\n",
        "plt.plot(epochs, val_loss, \"b--\",\n",
        "         label=\"Validation loss\")\n",
        "plt.title(\"Effect of insufficient model capacity on validation loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "691159d8",
      "metadata": {
        "id": "691159d8"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(96, activation=\"relu\"),\n",
        "    layers.Dense(96, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\"),\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_large_model = model.fit(\n",
        "    train_images, train_labels,\n",
        "    epochs=20,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09665a4b",
      "metadata": {
        "id": "09665a4b"
      },
      "source": [
        "## Improving generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf3dead",
      "metadata": {
        "id": "dcf3dead"
      },
      "source": [
        "### Dataset curation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b0725e",
      "metadata": {
        "id": "34b0725e"
      },
      "source": [
        "### Feature engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39cdf5f",
      "metadata": {
        "id": "e39cdf5f"
      },
      "source": [
        "### Using early stopping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fe8a1bc",
      "metadata": {
        "id": "3fe8a1bc"
      },
      "source": [
        "### Regularizing your model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2553860",
      "metadata": {
        "id": "a2553860"
      },
      "source": [
        "#### Reducing the network's size"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "024eb20f",
      "metadata": {
        "id": "024eb20f"
      },
      "source": [
        "**Original model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99ad962",
      "metadata": {
        "id": "e99ad962"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(train_data, train_labels), _ = imdb.load_data(num_words=10000)\n",
        "\n",
        "def vectorize_sequences(sequences, dimension=10000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        results[i, sequence] = 1.\n",
        "    return results\n",
        "train_data = vectorize_sequences(train_data)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_original = model.fit(train_data, train_labels,\n",
        "                             epochs=20, batch_size=512, validation_split=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e5b3d4",
      "metadata": {
        "id": "15e5b3d4"
      },
      "source": [
        "**Version of the model with lower capacity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffad1db",
      "metadata": {
        "id": "2ffad1db"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(4, activation=\"relu\"),\n",
        "    layers.Dense(4, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_smaller_model = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20, batch_size=512, validation_split=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9e8fa7d",
      "metadata": {
        "id": "d9e8fa7d"
      },
      "source": [
        "**Version of the model with higher capacity**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5af6e96",
      "metadata": {
        "id": "d5af6e96"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_larger_model = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20, batch_size=512, validation_split=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f093570",
      "metadata": {
        "id": "3f093570"
      },
      "source": [
        "#### Adding weight regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ae445f1",
      "metadata": {
        "id": "3ae445f1"
      },
      "source": [
        "**Adding L2 weight regularization to the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "882876c7",
      "metadata": {
        "id": "882876c7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(16,\n",
        "                 kernel_regularizer=regularizers.l2(0.002),\n",
        "                 activation=\"relu\"),\n",
        "    layers.Dense(16,\n",
        "                 kernel_regularizer=regularizers.l2(0.002),\n",
        "                 activation=\"relu\"),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_l2_reg = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20, batch_size=512, validation_split=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d027423",
      "metadata": {
        "id": "8d027423"
      },
      "source": [
        "**Different weight regularizers available in Keras**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0abae0b0",
      "metadata": {
        "id": "0abae0b0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import regularizers\n",
        "regularizers.l1(0.001)\n",
        "regularizers.l1_l2(l1=0.001, l2=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c1c53f7",
      "metadata": {
        "id": "9c1c53f7"
      },
      "source": [
        "#### Adding dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f8727d0",
      "metadata": {
        "id": "0f8727d0"
      },
      "source": [
        "**Adding dropout to the IMDB model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "debb8e94",
      "metadata": {
        "id": "debb8e94"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(16, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history_dropout = model.fit(\n",
        "    train_data, train_labels,\n",
        "    epochs=20, batch_size=512, validation_split=0.4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f21e1fe2",
      "metadata": {
        "id": "f21e1fe2"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b0159b1",
      "metadata": {
        "id": "0b0159b1"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8f7411",
      "metadata": {
        "id": "ff8f7411"
      },
      "source": [
        "# Working with Keras: A deep dive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57b6722a",
      "metadata": {
        "id": "57b6722a"
      },
      "source": [
        "## A spectrum of workflows"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8e49a6e",
      "metadata": {
        "id": "d8e49a6e"
      },
      "source": [
        "## Different ways to build Keras models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5a01947",
      "metadata": {
        "id": "d5a01947"
      },
      "source": [
        "### The Sequential model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c81dbe",
      "metadata": {
        "id": "19c81dbe"
      },
      "source": [
        "**The `Sequential` class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "685b9bba",
      "metadata": {
        "id": "685b9bba"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0f22598",
      "metadata": {
        "id": "d0f22598"
      },
      "source": [
        "**Incrementally building a Sequential model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2f449a7",
      "metadata": {
        "id": "f2f449a7"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(layers.Dense(64, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06053d0c",
      "metadata": {
        "id": "06053d0c"
      },
      "source": [
        "**Calling a model for the first time to build it**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7afc688d",
      "metadata": {
        "id": "7afc688d"
      },
      "outputs": [],
      "source": [
        "model.build(input_shape=(None, 3))\n",
        "model.weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7023f2c9",
      "metadata": {
        "id": "7023f2c9"
      },
      "source": [
        "**The summary method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6620a6f",
      "metadata": {
        "id": "d6620a6f"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a4abba",
      "metadata": {
        "id": "c0a4abba"
      },
      "source": [
        "**Naming models and layers with the `name` argument**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c19552d",
      "metadata": {
        "id": "4c19552d"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential(name=\"my_example_model\")\n",
        "model.add(layers.Dense(64, activation=\"relu\", name=\"my_first_layer\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\", name=\"my_last_layer\"))\n",
        "model.build((None, 3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af2a3aac",
      "metadata": {
        "id": "af2a3aac"
      },
      "source": [
        "**Specifying the input shape of your model in advance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d34484",
      "metadata": {
        "id": "b4d34484"
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "model.add(keras.Input(shape=(3,)))\n",
        "model.add(layers.Dense(64, activation=\"relu\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95073233",
      "metadata": {
        "id": "95073233"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d892d73",
      "metadata": {
        "id": "0d892d73"
      },
      "outputs": [],
      "source": [
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bf30db0",
      "metadata": {
        "id": "5bf30db0"
      },
      "source": [
        "### The Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72dde6f4",
      "metadata": {
        "id": "72dde6f4"
      },
      "source": [
        "#### A simple example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6882d83",
      "metadata": {
        "id": "c6882d83"
      },
      "source": [
        "**A simple Functional model with two `Dense` layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4d166dc",
      "metadata": {
        "id": "c4d166dc"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"my_input\")\n",
        "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb1ba35",
      "metadata": {
        "id": "ceb1ba35"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(3,), name=\"my_input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d20aacf",
      "metadata": {
        "id": "1d20aacf"
      },
      "outputs": [],
      "source": [
        "inputs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a77d5b70",
      "metadata": {
        "id": "a77d5b70"
      },
      "outputs": [],
      "source": [
        "inputs.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c01a9a96",
      "metadata": {
        "id": "c01a9a96"
      },
      "outputs": [],
      "source": [
        "features = layers.Dense(64, activation=\"relu\")(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5db5d61",
      "metadata": {
        "id": "b5db5d61"
      },
      "outputs": [],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6abdf2",
      "metadata": {
        "id": "8e6abdf2"
      },
      "outputs": [],
      "source": [
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5896416",
      "metadata": {
        "id": "f5896416"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbb09c4",
      "metadata": {
        "id": "cdbb09c4"
      },
      "source": [
        "#### Multi-input, multi-output models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "709fcadf",
      "metadata": {
        "id": "709fcadf"
      },
      "source": [
        "**A multi-input, multi-output Functional model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48f93155",
      "metadata": {
        "id": "48f93155"
      },
      "outputs": [],
      "source": [
        "vocabulary_size = 10000\n",
        "num_tags = 100\n",
        "num_departments = 4\n",
        "\n",
        "title = keras.Input(shape=(vocabulary_size,), name=\"title\")\n",
        "text_body = keras.Input(shape=(vocabulary_size,), name=\"text_body\")\n",
        "tags = keras.Input(shape=(num_tags,), name=\"tags\")\n",
        "\n",
        "features = layers.Concatenate()([title, text_body, tags])\n",
        "features = layers.Dense(64, activation=\"relu\")(features)\n",
        "\n",
        "priority = layers.Dense(1, activation=\"sigmoid\", name=\"priority\")(features)\n",
        "department = layers.Dense(\n",
        "    num_departments, activation=\"softmax\", name=\"department\")(features)\n",
        "\n",
        "model = keras.Model(inputs=[title, text_body, tags], outputs=[priority, department])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577560ba",
      "metadata": {
        "id": "577560ba"
      },
      "source": [
        "#### Training a multi-input, multi-output model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef3663bd",
      "metadata": {
        "id": "ef3663bd"
      },
      "source": [
        "**Training a model by providing lists of input & target arrays**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5b4106",
      "metadata": {
        "id": "9d5b4106"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "num_samples = 1280\n",
        "\n",
        "title_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "text_body_data = np.random.randint(0, 2, size=(num_samples, vocabulary_size))\n",
        "tags_data = np.random.randint(0, 2, size=(num_samples, num_tags))\n",
        "\n",
        "priority_data = np.random.random(size=(num_samples, 1))\n",
        "department_data = np.random.randint(0, 2, size=(num_samples, num_departments))\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
        "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
        "model.fit([title_data, text_body_data, tags_data],\n",
        "          [priority_data, department_data],\n",
        "          epochs=1)\n",
        "model.evaluate([title_data, text_body_data, tags_data],\n",
        "               [priority_data, department_data])\n",
        "priority_preds, department_preds = model.predict([title_data, text_body_data, tags_data])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3036d16b",
      "metadata": {
        "id": "3036d16b"
      },
      "source": [
        "**Training a model by providing dicts of input & target arrays**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da4df158",
      "metadata": {
        "id": "da4df158"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss={\"priority\": \"mean_squared_error\", \"department\": \"categorical_crossentropy\"},\n",
        "              metrics={\"priority\": [\"mean_absolute_error\"], \"department\": [\"accuracy\"]})\n",
        "model.fit({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "          {\"priority\": priority_data, \"department\": department_data},\n",
        "          epochs=1)\n",
        "model.evaluate({\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data},\n",
        "               {\"priority\": priority_data, \"department\": department_data})\n",
        "priority_preds, department_preds = model.predict(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1370b396",
      "metadata": {
        "id": "1370b396"
      },
      "source": [
        "#### The power of the Functional API: Access to layer connectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11b01fe8",
      "metadata": {
        "id": "11b01fe8"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"ticket_classifier.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0fc071",
      "metadata": {
        "id": "cf0fc071"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(model, \"ticket_classifier_with_shape_info.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4109a38a",
      "metadata": {
        "id": "4109a38a"
      },
      "source": [
        "**Retrieving the inputs or outputs of a layer in a Functional model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12aa2d1f",
      "metadata": {
        "id": "12aa2d1f"
      },
      "outputs": [],
      "source": [
        "model.layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c3313c",
      "metadata": {
        "id": "b2c3313c"
      },
      "outputs": [],
      "source": [
        "model.layers[3].input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea5d7842",
      "metadata": {
        "id": "ea5d7842"
      },
      "outputs": [],
      "source": [
        "model.layers[3].output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cda94c74",
      "metadata": {
        "id": "cda94c74"
      },
      "source": [
        "**Creating a new model by reusing intermediate layer outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54150fc0",
      "metadata": {
        "id": "54150fc0"
      },
      "outputs": [],
      "source": [
        "features = model.layers[4].output\n",
        "difficulty = layers.Dense(3, activation=\"softmax\", name=\"difficulty\")(features)\n",
        "\n",
        "new_model = keras.Model(\n",
        "    inputs=[title, text_body, tags],\n",
        "    outputs=[priority, department, difficulty])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76550746",
      "metadata": {
        "id": "76550746"
      },
      "outputs": [],
      "source": [
        "keras.utils.plot_model(new_model, \"updated_ticket_classifier.png\", show_shapes=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2367cbb",
      "metadata": {
        "id": "b2367cbb"
      },
      "source": [
        "### Subclassing the Model class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "707756bd",
      "metadata": {
        "id": "707756bd"
      },
      "source": [
        "#### Rewriting our previous example as a subclassed model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91149292",
      "metadata": {
        "id": "91149292"
      },
      "source": [
        "**A simple subclassed model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "26d2321f",
      "metadata": {
        "id": "26d2321f"
      },
      "outputs": [],
      "source": [
        "class CustomerTicketModel(keras.Model):\n",
        "\n",
        "    def __init__(self, num_departments):\n",
        "        super().__init__()\n",
        "        self.concat_layer = layers.Concatenate()\n",
        "        self.mixing_layer = layers.Dense(64, activation=\"relu\")\n",
        "        self.priority_scorer = layers.Dense(1, activation=\"sigmoid\")\n",
        "        self.department_classifier = layers.Dense(\n",
        "            num_departments, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs):\n",
        "        title = inputs[\"title\"]\n",
        "        text_body = inputs[\"text_body\"]\n",
        "        tags = inputs[\"tags\"]\n",
        "\n",
        "        features = self.concat_layer([title, text_body, tags])\n",
        "        features = self.mixing_layer(features)\n",
        "        priority = self.priority_scorer(features)\n",
        "        department = self.department_classifier(features)\n",
        "        return priority, department"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8a18e62",
      "metadata": {
        "id": "f8a18e62"
      },
      "outputs": [],
      "source": [
        "model = CustomerTicketModel(num_departments=4)\n",
        "\n",
        "priority, department = model(\n",
        "    {\"title\": title_data, \"text_body\": text_body_data, \"tags\": tags_data})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e4840c",
      "metadata": {
        "id": "69e4840c"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=[\"mean_squared_error\", \"categorical_crossentropy\"],\n",
        "              metrics=[[\"mean_absolute_error\"], [\"accuracy\"]])\n",
        "model.fit({\"title\": title_data,\n",
        "           \"text_body\": text_body_data,\n",
        "           \"tags\": tags_data},\n",
        "          [priority_data, department_data],\n",
        "          epochs=1)\n",
        "model.evaluate({\"title\": title_data,\n",
        "                \"text_body\": text_body_data,\n",
        "                \"tags\": tags_data},\n",
        "               [priority_data, department_data])\n",
        "priority_preds, department_preds = model.predict({\"title\": title_data,\n",
        "                                                  \"text_body\": text_body_data,\n",
        "                                                  \"tags\": tags_data})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e0656a2",
      "metadata": {
        "id": "7e0656a2"
      },
      "source": [
        "#### Beware: What subclassed models don't support"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9fc9e2",
      "metadata": {
        "id": "1d9fc9e2"
      },
      "source": [
        "### Mixing and matching different components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5299f95",
      "metadata": {
        "id": "a5299f95"
      },
      "source": [
        "**Creating a Functional model that includes a subclassed model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4994f2fb",
      "metadata": {
        "id": "4994f2fb"
      },
      "outputs": [],
      "source": [
        "class Classifier(keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        if num_classes == 2:\n",
        "            num_units = 1\n",
        "            activation = \"sigmoid\"\n",
        "        else:\n",
        "            num_units = num_classes\n",
        "            activation = \"softmax\"\n",
        "        self.dense = layers.Dense(num_units, activation=activation)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "\n",
        "inputs = keras.Input(shape=(3,))\n",
        "features = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "outputs = Classifier(num_classes=10)(features)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a88191",
      "metadata": {
        "id": "a9a88191"
      },
      "source": [
        "**Creating a subclassed model that includes a Functional model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ee4700",
      "metadata": {
        "id": "52ee4700"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(64,))\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(inputs)\n",
        "binary_classifier = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "class MyModel(keras.Model):\n",
        "\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.dense = layers.Dense(64, activation=\"relu\")\n",
        "        self.classifier = binary_classifier\n",
        "\n",
        "    def call(self, inputs):\n",
        "        features = self.dense(inputs)\n",
        "        return self.classifier(features)\n",
        "\n",
        "model = MyModel()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "473a427b",
      "metadata": {
        "id": "473a427b"
      },
      "source": [
        "### Remember: Use the right tool for the job"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f109ce35",
      "metadata": {
        "id": "f109ce35"
      },
      "source": [
        "## Using built-in training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e95986d7",
      "metadata": {
        "id": "e95986d7"
      },
      "source": [
        "**The standard workflow: `compile()`, `fit()`, `evaluate()`, `predict()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7becbf26",
      "metadata": {
        "id": "7becbf26"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "def get_mnist_model():\n",
        "    inputs = keras.Input(shape=(28 * 28,))\n",
        "    features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "    features = layers.Dropout(0.5)(features)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "(images, labels), (test_images, test_labels) = mnist.load_data()\n",
        "images = images.reshape((60000, 28 * 28)).astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28 * 28)).astype(\"float32\") / 255\n",
        "train_images, val_images = images[10000:], images[:10000]\n",
        "train_labels, val_labels = labels[10000:], labels[:10000]\n",
        "\n",
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=3,\n",
        "          validation_data=(val_images, val_labels))\n",
        "test_metrics = model.evaluate(test_images, test_labels)\n",
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81e0a2da",
      "metadata": {
        "id": "81e0a2da"
      },
      "source": [
        "### Writing your own metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be86c24b",
      "metadata": {
        "id": "be86c24b"
      },
      "source": [
        "**Implementing a custom metric by subclassing the `Metric` class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3878bdf0",
      "metadata": {
        "id": "3878bdf0"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class RootMeanSquaredError(keras.metrics.Metric):\n",
        "\n",
        "    def __init__(self, name=\"rmse\", **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.mse_sum = self.add_weight(name=\"mse_sum\", initializer=\"zeros\")\n",
        "        self.total_samples = self.add_weight(\n",
        "            name=\"total_samples\", initializer=\"zeros\", dtype=\"int32\")\n",
        "\n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.one_hot(y_true, depth=tf.shape(y_pred)[1])\n",
        "        mse = tf.reduce_sum(tf.square(y_true - y_pred))\n",
        "        self.mse_sum.assign_add(mse)\n",
        "        num_samples = tf.shape(y_pred)[0]\n",
        "        self.total_samples.assign_add(num_samples)\n",
        "\n",
        "    def result(self):\n",
        "        return tf.sqrt(self.mse_sum / tf.cast(self.total_samples, tf.float32))\n",
        "\n",
        "    def reset_state(self):\n",
        "        self.mse_sum.assign(0.)\n",
        "        self.total_samples.assign(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89b8debb",
      "metadata": {
        "id": "89b8debb"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\", RootMeanSquaredError()])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=3,\n",
        "          validation_data=(val_images, val_labels))\n",
        "test_metrics = model.evaluate(test_images, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3afb04bf",
      "metadata": {
        "id": "3afb04bf"
      },
      "source": [
        "### Using callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b06be8b5",
      "metadata": {
        "id": "b06be8b5"
      },
      "source": [
        "#### The EarlyStopping and ModelCheckpoint callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc415ae",
      "metadata": {
        "id": "ebc415ae"
      },
      "source": [
        "**Using the `callbacks` argument in the `fit()` method**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e13054",
      "metadata": {
        "id": "00e13054"
      },
      "outputs": [],
      "source": [
        "callbacks_list = [\n",
        "    keras.callbacks.EarlyStopping(\n",
        "        monitor=\"val_accuracy\",\n",
        "        patience=2,\n",
        "    ),\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"checkpoint_path.keras\",\n",
        "        monitor=\"val_loss\",\n",
        "        save_best_only=True,\n",
        "    )\n",
        "]\n",
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          callbacks=callbacks_list,\n",
        "          validation_data=(val_images, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a459941",
      "metadata": {
        "id": "5a459941"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"checkpoint_path.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a08bf10",
      "metadata": {
        "id": "9a08bf10"
      },
      "source": [
        "### Writing your own callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5133de23",
      "metadata": {
        "id": "5133de23"
      },
      "source": [
        "**Creating a custom callback by subclassing the `Callback` class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2182767c",
      "metadata": {
        "id": "2182767c"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "class LossHistory(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs):\n",
        "        self.per_batch_losses = []\n",
        "\n",
        "    def on_batch_end(self, batch, logs):\n",
        "        self.per_batch_losses.append(logs.get(\"loss\"))\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        plt.clf()\n",
        "        plt.plot(range(len(self.per_batch_losses)), self.per_batch_losses,\n",
        "                 label=\"Training loss for each batch\")\n",
        "        plt.xlabel(f\"Batch (epoch {epoch})\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.legend()\n",
        "        plt.savefig(f\"plot_at_epoch_{epoch}\")\n",
        "        self.per_batch_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1385e96f",
      "metadata": {
        "id": "1385e96f"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          callbacks=[LossHistory()],\n",
        "          validation_data=(val_images, val_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d673ba0f",
      "metadata": {
        "id": "d673ba0f"
      },
      "source": [
        "### Monitoring and visualization with TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0feb612d",
      "metadata": {
        "id": "0feb612d"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "tensorboard = keras.callbacks.TensorBoard(\n",
        "    log_dir=\"/full_path_to_your_log_dir\",\n",
        ")\n",
        "model.fit(train_images, train_labels,\n",
        "          epochs=10,\n",
        "          validation_data=(val_images, val_labels),\n",
        "          callbacks=[tensorboard])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578e34a7",
      "metadata": {
        "id": "578e34a7"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /full_path_to_your_log_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6409af08",
      "metadata": {
        "id": "6409af08"
      },
      "source": [
        "## Writing your own training and evaluation loops"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f9ab164",
      "metadata": {
        "id": "9f9ab164"
      },
      "source": [
        "### Training versus inference"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89df5776",
      "metadata": {
        "id": "89df5776"
      },
      "source": [
        "### Low-level usage of metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4961390b",
      "metadata": {
        "id": "4961390b"
      },
      "outputs": [],
      "source": [
        "metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "targets = [0, 1, 2]\n",
        "predictions = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "metric.update_state(targets, predictions)\n",
        "current_result = metric.result()\n",
        "print(f\"result: {current_result:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c813be74",
      "metadata": {
        "id": "c813be74"
      },
      "outputs": [],
      "source": [
        "values = [0, 1, 2, 3, 4]\n",
        "mean_tracker = keras.metrics.Mean()\n",
        "for value in values:\n",
        "    mean_tracker.update_state(value)\n",
        "print(f\"Mean of values: {mean_tracker.result():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba428c40",
      "metadata": {
        "id": "ba428c40"
      },
      "source": [
        "### A complete training and evaluation loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9f78ed4",
      "metadata": {
        "id": "f9f78ed4"
      },
      "source": [
        "**Writing a step-by-step training loop: the training step function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86c5fe70",
      "metadata": {
        "id": "86c5fe70"
      },
      "outputs": [],
      "source": [
        "model = get_mnist_model()\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "optimizer = keras.optimizers.RMSprop()\n",
        "metrics = [keras.metrics.SparseCategoricalAccuracy()]\n",
        "loss_tracking_metric = keras.metrics.Mean()\n",
        "\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs, training=True)\n",
        "        loss = loss_fn(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"loss\"] = loss_tracking_metric.result()\n",
        "    return logs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7867cdc5",
      "metadata": {
        "id": "7867cdc5"
      },
      "source": [
        "**Writing a step-by-step training loop: resetting the metrics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f86321",
      "metadata": {
        "id": "17f86321"
      },
      "outputs": [],
      "source": [
        "def reset_metrics():\n",
        "    for metric in metrics:\n",
        "        metric.reset_state()\n",
        "    loss_tracking_metric.reset_state()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69cf43d",
      "metadata": {
        "id": "f69cf43d"
      },
      "source": [
        "**Writing a step-by-step training loop: the loop itself**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67de2006",
      "metadata": {
        "id": "67de2006"
      },
      "outputs": [],
      "source": [
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "training_dataset = training_dataset.batch(32)\n",
        "epochs = 3\n",
        "for epoch in range(epochs):\n",
        "    reset_metrics()\n",
        "    for inputs_batch, targets_batch in training_dataset:\n",
        "        logs = train_step(inputs_batch, targets_batch)\n",
        "    print(f\"Results at the end of epoch {epoch}\")\n",
        "    for key, value in logs.items():\n",
        "        print(f\"...{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ed9a8c",
      "metadata": {
        "id": "91ed9a8c"
      },
      "source": [
        "**Writing a step-by-step evaluation loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2b4b21",
      "metadata": {
        "id": "6e2b4b21"
      },
      "outputs": [],
      "source": [
        "def test_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[\"val_\" + metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "    return logs\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "    logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "472fa45d",
      "metadata": {
        "id": "472fa45d"
      },
      "source": [
        "### Make it fast with tf.function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccb25bc8",
      "metadata": {
        "id": "ccb25bc8"
      },
      "source": [
        "**Adding a `tf.function` decorator to our evaluation-step function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af881779",
      "metadata": {
        "id": "af881779"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(inputs, targets):\n",
        "    predictions = model(inputs, training=False)\n",
        "    loss = loss_fn(targets, predictions)\n",
        "\n",
        "    logs = {}\n",
        "    for metric in metrics:\n",
        "        metric.update_state(targets, predictions)\n",
        "        logs[\"val_\" + metric.name] = metric.result()\n",
        "\n",
        "    loss_tracking_metric.update_state(loss)\n",
        "    logs[\"val_loss\"] = loss_tracking_metric.result()\n",
        "    return logs\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_images, val_labels))\n",
        "val_dataset = val_dataset.batch(32)\n",
        "reset_metrics()\n",
        "for inputs_batch, targets_batch in val_dataset:\n",
        "    logs = test_step(inputs_batch, targets_batch)\n",
        "print(\"Evaluation results:\")\n",
        "for key, value in logs.items():\n",
        "    print(f\"...{key}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ccd875c",
      "metadata": {
        "id": "4ccd875c"
      },
      "source": [
        "### Leveraging fit() with a custom training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8edf4d63",
      "metadata": {
        "id": "8edf4d63"
      },
      "source": [
        "**Implementing a custom training step to use with `fit()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4bf05bd",
      "metadata": {
        "id": "e4bf05bd"
      },
      "outputs": [],
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy()\n",
        "loss_tracker = keras.metrics.Mean(name=\"loss\")\n",
        "\n",
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = loss_fn(targets, predictions)\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "\n",
        "        loss_tracker.update_state(loss)\n",
        "        return {\"loss\": loss_tracker.result()}\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [loss_tracker]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6a3137",
      "metadata": {
        "id": "5e6a3137"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop())\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ef63b83",
      "metadata": {
        "id": "4ef63b83"
      },
      "outputs": [],
      "source": [
        "class CustomModel(keras.Model):\n",
        "    def train_step(self, data):\n",
        "        inputs, targets = data\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self(inputs, training=True)\n",
        "            loss = self.compiled_loss(targets, predictions)\n",
        "        gradients = tape.gradient(loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "        self.compiled_metrics.update_state(targets, predictions)\n",
        "        return {m.name: m.result() for m in self.metrics}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa69e6d",
      "metadata": {
        "id": "bfa69e6d"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(28 * 28,))\n",
        "features = layers.Dense(512, activation=\"relu\")(inputs)\n",
        "features = layers.Dropout(0.5)(features)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(features)\n",
        "model = CustomModel(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=keras.optimizers.RMSprop(),\n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(),\n",
        "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
        "model.fit(train_images, train_labels, epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09d70331",
      "metadata": {
        "id": "09d70331"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1990b4bd",
      "metadata": {
        "id": "1990b4bd"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c9babdf",
      "metadata": {
        "id": "4c9babdf"
      },
      "source": [
        "# Introduction to deep learning for computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d0cf8ae",
      "metadata": {
        "id": "3d0cf8ae"
      },
      "source": [
        "## Introduction to convnets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a88e5368",
      "metadata": {
        "id": "a88e5368"
      },
      "source": [
        "**Instantiating a small convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959b33b4",
      "metadata": {
        "id": "959b33b4"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19c5a6d0",
      "metadata": {
        "id": "19c5a6d0"
      },
      "source": [
        "**Displaying the model's summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ead93bc",
      "metadata": {
        "id": "3ead93bc"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a925df",
      "metadata": {
        "id": "39a925df"
      },
      "source": [
        "**Training the convnet on MNIST images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3b90ee",
      "metadata": {
        "id": "4c3b90ee"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "train_images = train_images.reshape((60000, 28, 28, 1))\n",
        "train_images = train_images.astype(\"float32\") / 255\n",
        "test_images = test_images.reshape((10000, 28, 28, 1))\n",
        "test_images = test_images.astype(\"float32\") / 255\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "model.fit(train_images, train_labels, epochs=5, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1f925d",
      "metadata": {
        "id": "1b1f925d"
      },
      "source": [
        "**Evaluating the convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38373037",
      "metadata": {
        "id": "38373037"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a1b5e2c",
      "metadata": {
        "id": "3a1b5e2c"
      },
      "source": [
        "### The convolution operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7ecd05",
      "metadata": {
        "id": "9d7ecd05"
      },
      "source": [
        "#### Understanding border effects and padding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2538e7dd",
      "metadata": {
        "id": "2538e7dd"
      },
      "source": [
        "#### Understanding convolution strides"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d67362",
      "metadata": {
        "id": "85d67362"
      },
      "source": [
        "### The max-pooling operation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f411b4",
      "metadata": {
        "id": "57f411b4"
      },
      "source": [
        "**An incorrectly structured convnet missing its max-pooling layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b52998b8",
      "metadata": {
        "id": "b52998b8"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a5ccae",
      "metadata": {
        "id": "06a5ccae"
      },
      "outputs": [],
      "source": [
        "model_no_max_pool.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63ec048b",
      "metadata": {
        "id": "63ec048b"
      },
      "source": [
        "## Training a convnet from scratch on a small dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05eac96f",
      "metadata": {
        "id": "05eac96f"
      },
      "source": [
        "### The relevance of deep learning for small-data problems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b31da3",
      "metadata": {
        "id": "39b31da3"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "726643ff",
      "metadata": {
        "id": "726643ff"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e077c94e",
      "metadata": {
        "id": "e077c94e"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e92570",
      "metadata": {
        "id": "60e92570"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacabd82",
      "metadata": {
        "id": "cacabd82"
      },
      "outputs": [],
      "source": [
        "!unzip -qq dogs-vs-cats.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a41c3b7",
      "metadata": {
        "id": "5a41c3b7"
      },
      "outputs": [],
      "source": [
        "!unzip -qq train.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f06a290",
      "metadata": {
        "id": "6f06a290"
      },
      "source": [
        "**Copying images to training, validation, and test directories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51af6387",
      "metadata": {
        "id": "51af6387"
      },
      "outputs": [],
      "source": [
        "import os, shutil, pathlib\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c4a2423",
      "metadata": {
        "id": "2c4a2423"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a19c594",
      "metadata": {
        "id": "7a19c594"
      },
      "source": [
        "**Instantiating a small convnet for dogs vs. cats classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "829687bd",
      "metadata": {
        "id": "829687bd"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c6d72c",
      "metadata": {
        "id": "c4c6d72c"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb49fa77",
      "metadata": {
        "id": "fb49fa77"
      },
      "source": [
        "**Configuring the model for training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d0c1258",
      "metadata": {
        "id": "1d0c1258"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb9ff6c7",
      "metadata": {
        "id": "eb9ff6c7"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7a4b588",
      "metadata": {
        "id": "f7a4b588"
      },
      "source": [
        "**Using `image_dataset_from_directory` to read images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a853247a",
      "metadata": {
        "id": "a853247a"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f33ffe",
      "metadata": {
        "id": "95f33ffe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba81e201",
      "metadata": {
        "id": "ba81e201"
      },
      "outputs": [],
      "source": [
        "for i, element in enumerate(dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b46fc0c",
      "metadata": {
        "id": "4b46fc0c"
      },
      "outputs": [],
      "source": [
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b9d42f",
      "metadata": {
        "id": "38b9d42f"
      },
      "outputs": [],
      "source": [
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "    print(element.shape)\n",
        "    if i >= 2:\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f6be69b",
      "metadata": {
        "id": "1f6be69b"
      },
      "source": [
        "**Displaying the shapes of the data and labels yielded by the `Dataset`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f6a1e23",
      "metadata": {
        "id": "4f6a1e23"
      },
      "outputs": [],
      "source": [
        "for data_batch, labels_batch in train_dataset:\n",
        "    print(\"data batch shape:\", data_batch.shape)\n",
        "    print(\"labels batch shape:\", labels_batch.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ed3d18e",
      "metadata": {
        "id": "1ed3d18e"
      },
      "source": [
        "**Fitting the model using a `Dataset`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aed0cb1c",
      "metadata": {
        "id": "aed0cb1c"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b982400b",
      "metadata": {
        "id": "b982400b"
      },
      "source": [
        "**Displaying curves of loss and accuracy during training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a18f27cf",
      "metadata": {
        "id": "a18f27cf"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a53e13d",
      "metadata": {
        "id": "8a53e13d"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b06072c",
      "metadata": {
        "id": "3b06072c"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "991284df",
      "metadata": {
        "id": "991284df"
      },
      "source": [
        "### Using data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d654c88d",
      "metadata": {
        "id": "d654c88d"
      },
      "source": [
        "**Define a data augmentation stage to add to an image model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0180700",
      "metadata": {
        "id": "e0180700"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111c7297",
      "metadata": {
        "id": "111c7297"
      },
      "source": [
        "**Displaying some randomly augmented training images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e383120",
      "metadata": {
        "id": "0e383120"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_dataset.take(1):\n",
        "    for i in range(9):\n",
        "        augmented_images = data_augmentation(images)\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "        plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1fafb37",
      "metadata": {
        "id": "f1fafb37"
      },
      "source": [
        "**Defining a new convnet that includes image augmentation and dropout**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a07d0c5c",
      "metadata": {
        "id": "a07d0c5c"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0331a37c",
      "metadata": {
        "id": "0331a37c"
      },
      "source": [
        "**Training the regularized convnet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b083890",
      "metadata": {
        "id": "8b083890"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aec63cb",
      "metadata": {
        "id": "1aec63cb"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6415d7a2",
      "metadata": {
        "id": "6415d7a2"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cc7107",
      "metadata": {
        "id": "c0cc7107"
      },
      "source": [
        "## Leveraging a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d3cf86",
      "metadata": {
        "id": "04d3cf86"
      },
      "source": [
        "### Feature extraction with a pretrained model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4379380b",
      "metadata": {
        "id": "4379380b"
      },
      "source": [
        "**Instantiating the VGG16 convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a56000d2",
      "metadata": {
        "id": "a56000d2"
      },
      "outputs": [],
      "source": [
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,\n",
        "    input_shape=(180, 180, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e72c263c",
      "metadata": {
        "id": "e72c263c"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "139180ee",
      "metadata": {
        "id": "139180ee"
      },
      "source": [
        "#### Fast feature extraction without data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36d21edc",
      "metadata": {
        "id": "36d21edc"
      },
      "source": [
        "**Extracting the VGG16 features and corresponding labels**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c74ae828",
      "metadata": {
        "id": "c74ae828"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_features_and_labels(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "    for images, labels in dataset:\n",
        "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "        features = conv_base.predict(preprocessed_images)\n",
        "        all_features.append(features)\n",
        "        all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
        "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels =  get_features_and_labels(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eab2fdab",
      "metadata": {
        "id": "eab2fdab"
      },
      "outputs": [],
      "source": [
        "train_features.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3391b1e",
      "metadata": {
        "id": "e3391b1e"
      },
      "source": [
        "**Defining and training the densely connected classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10480f3e",
      "metadata": {
        "id": "10480f3e"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_features, train_labels,\n",
        "    epochs=20,\n",
        "    validation_data=(val_features, val_labels),\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "649e25b8",
      "metadata": {
        "id": "649e25b8"
      },
      "source": [
        "**Plotting the results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5e5740",
      "metadata": {
        "id": "3d5e5740"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d1339e",
      "metadata": {
        "id": "a6d1339e"
      },
      "source": [
        "#### Feature extraction together with data augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2df6cc29",
      "metadata": {
        "id": "2df6cc29"
      },
      "source": [
        "**Instantiating and freezing the VGG16 convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef437f68",
      "metadata": {
        "id": "ef437f68"
      },
      "outputs": [],
      "source": [
        "conv_base  = keras.applications.vgg16.VGG16(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)\n",
        "conv_base.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a257805",
      "metadata": {
        "id": "0a257805"
      },
      "source": [
        "**Printing the list of trainable weights before and after freezing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e6b73b7",
      "metadata": {
        "id": "3e6b73b7"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"before freezing the conv base:\", len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3d3e3d9",
      "metadata": {
        "id": "e3d3e3d9"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = False\n",
        "print(\"This is the number of trainable weights \"\n",
        "      \"after freezing the conv base:\", len(conv_base.trainable_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04643d7a",
      "metadata": {
        "id": "04643d7a"
      },
      "source": [
        "**Adding a data augmentation stage and a classifier to the convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2a0168d",
      "metadata": {
        "id": "b2a0168d"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "080eeb07",
      "metadata": {
        "id": "080eeb07"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69cbd056",
      "metadata": {
        "id": "69cbd056"
      },
      "source": [
        "**Evaluating the model on the test set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4240cab7",
      "metadata": {
        "id": "4240cab7"
      },
      "outputs": [],
      "source": [
        "test_model = keras.models.load_model(\n",
        "    \"feature_extraction_with_data_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8df016eb",
      "metadata": {
        "id": "8df016eb"
      },
      "source": [
        "### Fine-tuning a pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84fb4512",
      "metadata": {
        "id": "84fb4512"
      },
      "outputs": [],
      "source": [
        "conv_base.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb10be6",
      "metadata": {
        "id": "2fb10be6"
      },
      "source": [
        "**Freezing all layers until the fourth from the last**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f780590",
      "metadata": {
        "id": "8f780590"
      },
      "outputs": [],
      "source": [
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fc909b4",
      "metadata": {
        "id": "0fc909b4"
      },
      "source": [
        "**Fine-tuning the model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48782b63",
      "metadata": {
        "id": "48782b63"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"fine_tuning.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")\n",
        "]\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fa5f00",
      "metadata": {
        "id": "94fa5f00"
      },
      "outputs": [],
      "source": [
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1ea093",
      "metadata": {
        "id": "9b1ea093"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74870e8a",
      "metadata": {
        "id": "74870e8a"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "730dc40c",
      "metadata": {
        "id": "730dc40c"
      },
      "source": [
        "# Advanced deep learning for computer vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31baf169",
      "metadata": {
        "id": "31baf169"
      },
      "source": [
        "## Three essential computer vision tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0db2763c",
      "metadata": {
        "id": "0db2763c"
      },
      "source": [
        "## An image segmentation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39f408d",
      "metadata": {
        "id": "d39f408d"
      },
      "outputs": [],
      "source": [
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33332165",
      "metadata": {
        "id": "33332165"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [os.path.join(input_dir, fname)\n",
        "     for fname in os.listdir(input_dir)\n",
        "     if fname.endswith(\".jpg\")])\n",
        "target_paths = sorted(\n",
        "    [os.path.join(target_dir, fname)\n",
        "     for fname in os.listdir(target_dir)\n",
        "     if fname.endswith(\".png\") and not fname.startswith(\".\")])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3e7da17",
      "metadata": {
        "id": "e3e7da17"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(load_img(input_img_paths[9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0190819f",
      "metadata": {
        "id": "0190819f"
      },
      "outputs": [],
      "source": [
        "def display_target(target_array):\n",
        "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(normalized_array[:, :, 0])\n",
        "\n",
        "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
        "display_target(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "016a9a2c",
      "metadata": {
        "id": "016a9a2c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "img_size = (200, 200)\n",
        "num_imgs = len(input_img_paths)\n",
        "\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_paths)\n",
        "\n",
        "def path_to_input_image(path):\n",
        "    return img_to_array(load_img(path, target_size=img_size))\n",
        "\n",
        "def path_to_target(path):\n",
        "    img = img_to_array(\n",
        "        load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
        "    img = img.astype(\"uint8\") - 1\n",
        "    return img\n",
        "\n",
        "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
        "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
        "for i in range(num_imgs):\n",
        "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
        "    targets[i] = path_to_target(target_paths[i])\n",
        "\n",
        "num_val_samples = 1000\n",
        "train_input_imgs = input_imgs[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_input_imgs = input_imgs[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a5e3f2",
      "metadata": {
        "id": "d1a5e3f2"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "model = get_model(img_size=img_size, num_classes=3)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0520888d",
      "metadata": {
        "id": "0520888d"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "history = model.fit(train_input_imgs, train_targets,\n",
        "                    epochs=50,\n",
        "                    callbacks=callbacks,\n",
        "                    batch_size=64,\n",
        "                    validation_data=(val_input_imgs, val_targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "430f78af",
      "metadata": {
        "id": "430f78af"
      },
      "outputs": [],
      "source": [
        "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b9e0605",
      "metadata": {
        "id": "0b9e0605"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import array_to_img\n",
        "\n",
        "model = keras.models.load_model(\"oxford_segmentation.keras\")\n",
        "\n",
        "i = 4\n",
        "test_image = val_input_imgs[i]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(array_to_img(test_image))\n",
        "\n",
        "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
        "\n",
        "def display_mask(pred):\n",
        "    mask = np.argmax(pred, axis=-1)\n",
        "    mask *= 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(mask)\n",
        "\n",
        "display_mask(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b683680",
      "metadata": {
        "id": "4b683680"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2091a97f",
      "metadata": {
        "id": "2091a97f"
      },
      "source": [
        "## Modern convnet architecture patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b07e37d5",
      "metadata": {
        "id": "b07e37d5"
      },
      "source": [
        "### Modularity, hierarchy, and reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5b9c194",
      "metadata": {
        "id": "e5b9c194"
      },
      "source": [
        "### Residual connections"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24c90f38",
      "metadata": {
        "id": "24c90f38"
      },
      "source": [
        "**Residual block where the number of filters changes**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ba0c61",
      "metadata": {
        "id": "57ba0c61"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "residual = x\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "residual = layers.Conv2D(64, 1)(residual)\n",
        "x = layers.add([x, residual])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40359d48",
      "metadata": {
        "id": "40359d48"
      },
      "source": [
        "**Case where target block includes a max pooling layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6964dd6f",
      "metadata": {
        "id": "6964dd6f"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "residual = x\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
        "residual = layers.Conv2D(64, 1, strides=2)(residual)\n",
        "x = layers.add([x, residual])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bbe49b5",
      "metadata": {
        "id": "5bbe49b5"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(32, 32, 3))\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "def residual_block(x, filters, pooling=False):\n",
        "    residual = x\n",
        "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    if pooling:\n",
        "        x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
        "        residual = layers.Conv2D(filters, 1, strides=2)(residual)\n",
        "    elif filters != residual.shape[-1]:\n",
        "        residual = layers.Conv2D(filters, 1)(residual)\n",
        "    x = layers.add([x, residual])\n",
        "    return x\n",
        "\n",
        "x = residual_block(x, filters=32, pooling=True)\n",
        "x = residual_block(x, filters=64, pooling=True)\n",
        "x = residual_block(x, filters=128, pooling=False)\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f45fd8",
      "metadata": {
        "id": "a2f45fd8"
      },
      "source": [
        "### Batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6350bde2",
      "metadata": {
        "id": "6350bde2"
      },
      "source": [
        "### Depthwise separable convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd0b54bd",
      "metadata": {
        "id": "cd0b54bd"
      },
      "source": [
        "### Putting it together: A mini Xception-like model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d0a450d",
      "metadata": {
        "id": "7d0a450d"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9021d0a6",
      "metadata": {
        "id": "9021d0a6"
      },
      "outputs": [],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c dogs-vs-cats\n",
        "!unzip -qq train.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f39bf804",
      "metadata": {
        "id": "f39bf804"
      },
      "outputs": [],
      "source": [
        "import os, shutil, pathlib\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "\n",
        "original_dir = pathlib.Path(\"train\")\n",
        "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
        "\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "    for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "            shutil.copyfile(src=original_dir / fname,\n",
        "                            dst=dir / fname)\n",
        "\n",
        "make_subset(\"train\", start_index=0, end_index=1000)\n",
        "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "make_subset(\"test\", start_index=1500, end_index=2500)\n",
        "\n",
        "train_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"train\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"validation\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    new_base_dir / \"test\",\n",
        "    image_size=(180, 180),\n",
        "    batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1b6486",
      "metadata": {
        "id": "fc1b6486"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50326d7b",
      "metadata": {
        "id": "50326d7b"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
        "\n",
        "for size in [32, 64, 128, 256, 512]:\n",
        "    residual = x\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
        "\n",
        "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "    residual = layers.Conv2D(\n",
        "        size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
        "    x = layers.add([x, residual])\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2713a35",
      "metadata": {
        "id": "a2713a35"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=100,\n",
        "    validation_data=validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6f30fc6",
      "metadata": {
        "id": "f6f30fc6"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40ccd3cb",
      "metadata": {
        "id": "40ccd3cb"
      },
      "source": [
        "## Interpreting what convnets learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c74629",
      "metadata": {
        "id": "44c74629"
      },
      "source": [
        "### Visualizing intermediate activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13cb4e87",
      "metadata": {
        "id": "13cb4e87"
      },
      "outputs": [],
      "source": [
        "# You can use this to load the file \"convnet_from_scratch_with_augmentation.keras\"\n",
        "# you obtained in the last chapter.\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d19db712",
      "metadata": {
        "id": "d19db712"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73a32d6a",
      "metadata": {
        "id": "73a32d6a"
      },
      "source": [
        "**Preprocessing a single image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db047f9e",
      "metadata": {
        "id": "db047f9e"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "\n",
        "img_path = keras.utils.get_file(\n",
        "    fname=\"cat.jpg\",\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/cat.jpg\")\n",
        "\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = keras.utils.load_img(\n",
        "        img_path, target_size=target_size)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "img_tensor = get_img_array(img_path, target_size=(180, 180))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58a9f4d0",
      "metadata": {
        "id": "58a9f4d0"
      },
      "source": [
        "**Displaying the test picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4dd8013",
      "metadata": {
        "id": "b4dd8013"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be999a32",
      "metadata": {
        "id": "be999a32"
      },
      "source": [
        "**Instantiating a model that returns layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a96898e7",
      "metadata": {
        "id": "a96898e7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "layer_outputs = []\n",
        "layer_names = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, (layers.Conv2D, layers.MaxPooling2D)):\n",
        "        layer_outputs.append(layer.output)\n",
        "        layer_names.append(layer.name)\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f211c4",
      "metadata": {
        "id": "f5f211c4"
      },
      "source": [
        "**Using the model to compute layer activations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02ba01f",
      "metadata": {
        "id": "a02ba01f"
      },
      "outputs": [],
      "source": [
        "activations = activation_model.predict(img_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6742a41",
      "metadata": {
        "id": "f6742a41"
      },
      "outputs": [],
      "source": [
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55bd5ef3",
      "metadata": {
        "id": "55bd5ef3"
      },
      "source": [
        "**Visualizing the fifth channel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f72aa875",
      "metadata": {
        "id": "f72aa875"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.matshow(first_layer_activation[0, :, :, 5], cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "458a9ea6",
      "metadata": {
        "id": "458a9ea6"
      },
      "source": [
        "**Visualizing every channel in every intermediate activation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef93672d",
      "metadata": {
        "id": "ef93672d"
      },
      "outputs": [],
      "source": [
        "images_per_row = 16\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    n_features = layer_activation.shape[-1]\n",
        "    size = layer_activation.shape[1]\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros(((size + 1) * n_cols - 1,\n",
        "                             images_per_row * (size + 1) - 1))\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_index = col * images_per_row + row\n",
        "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
        "            if channel_image.sum() != 0:\n",
        "                channel_image -= channel_image.mean()\n",
        "                channel_image /= channel_image.std()\n",
        "                channel_image *= 64\n",
        "                channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
        "            display_grid[\n",
        "                col * (size + 1): (col + 1) * size + col,\n",
        "                row * (size + 1) : (row + 1) * size + row] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f104bbe",
      "metadata": {
        "id": "4f104bbe"
      },
      "source": [
        "### Visualizing convnet filters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e815a92",
      "metadata": {
        "id": "8e815a92"
      },
      "source": [
        "**Instantiating the Xception convolutional base**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95f9ea2d",
      "metadata": {
        "id": "95f9ea2d"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a20ba857",
      "metadata": {
        "id": "a20ba857"
      },
      "source": [
        "**Printing the names of all convolutional layers in Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b11ef65a",
      "metadata": {
        "id": "b11ef65a"
      },
      "outputs": [],
      "source": [
        "for layer in model.layers:\n",
        "    if isinstance(layer, (keras.layers.Conv2D, keras.layers.SeparableConv2D)):\n",
        "        print(layer.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2e1b31",
      "metadata": {
        "id": "7c2e1b31"
      },
      "source": [
        "**Creating a feature extractor model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dcf9815",
      "metadata": {
        "id": "8dcf9815"
      },
      "outputs": [],
      "source": [
        "layer_name = \"block3_sepconv1\"\n",
        "layer = model.get_layer(name=layer_name)\n",
        "feature_extractor = keras.Model(inputs=model.input, outputs=layer.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2d2d5f0",
      "metadata": {
        "id": "a2d2d5f0"
      },
      "source": [
        "**Using the feature extractor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df63a329",
      "metadata": {
        "id": "df63a329"
      },
      "outputs": [],
      "source": [
        "activation = feature_extractor(\n",
        "    keras.applications.xception.preprocess_input(img_tensor)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e423bae7",
      "metadata": {
        "id": "e423bae7"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def compute_loss(image, filter_index):\n",
        "    activation = feature_extractor(image)\n",
        "    filter_activation = activation[:, 2:-2, 2:-2, filter_index]\n",
        "    return tf.reduce_mean(filter_activation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae748117",
      "metadata": {
        "id": "ae748117"
      },
      "source": [
        "**Loss maximization via stochastic gradient ascent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e943cc16",
      "metadata": {
        "id": "e943cc16"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def gradient_ascent_step(image, filter_index, learning_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        loss = compute_loss(image, filter_index)\n",
        "    grads = tape.gradient(loss, image)\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    image += learning_rate * grads\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4010b3c3",
      "metadata": {
        "id": "4010b3c3"
      },
      "source": [
        "**Function to generate filter visualizations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b386064",
      "metadata": {
        "id": "3b386064"
      },
      "outputs": [],
      "source": [
        "img_width = 200\n",
        "img_height = 200\n",
        "\n",
        "def generate_filter_pattern(filter_index):\n",
        "    iterations = 30\n",
        "    learning_rate = 10.\n",
        "    image = tf.random.uniform(\n",
        "        minval=0.4,\n",
        "        maxval=0.6,\n",
        "        shape=(1, img_width, img_height, 3))\n",
        "    for i in range(iterations):\n",
        "        image = gradient_ascent_step(image, filter_index, learning_rate)\n",
        "    return image[0].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8f41363",
      "metadata": {
        "id": "f8f41363"
      },
      "source": [
        "**Utility function to convert a tensor into a valid image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712e272f",
      "metadata": {
        "id": "712e272f"
      },
      "outputs": [],
      "source": [
        "def deprocess_image(image):\n",
        "    image -= image.mean()\n",
        "    image /= image.std()\n",
        "    image *= 64\n",
        "    image += 128\n",
        "    image = np.clip(image, 0, 255).astype(\"uint8\")\n",
        "    image = image[25:-25, 25:-25, :]\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23770364",
      "metadata": {
        "id": "23770364"
      },
      "outputs": [],
      "source": [
        "plt.axis(\"off\")\n",
        "plt.imshow(deprocess_image(generate_filter_pattern(filter_index=2)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f298fe",
      "metadata": {
        "id": "95f298fe"
      },
      "source": [
        "**Generating a grid of all filter response patterns in a layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5af63d0",
      "metadata": {
        "id": "e5af63d0"
      },
      "outputs": [],
      "source": [
        "all_images = []\n",
        "for filter_index in range(64):\n",
        "    print(f\"Processing filter {filter_index}\")\n",
        "    image = deprocess_image(\n",
        "        generate_filter_pattern(filter_index)\n",
        "    )\n",
        "    all_images.append(image)\n",
        "\n",
        "margin = 5\n",
        "n = 8\n",
        "cropped_width = img_width - 25 * 2\n",
        "cropped_height = img_height - 25 * 2\n",
        "width = n * cropped_width + (n - 1) * margin\n",
        "height = n * cropped_height + (n - 1) * margin\n",
        "stitched_filters = np.zeros((width, height, 3))\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        image = all_images[i * n + j]\n",
        "        stitched_filters[\n",
        "            (cropped_width + margin) * i : (cropped_width + margin) * i + cropped_width,\n",
        "            (cropped_height + margin) * j : (cropped_height + margin) * j\n",
        "            + cropped_height,\n",
        "            :,\n",
        "        ] = image\n",
        "\n",
        "keras.utils.save_img(\n",
        "    f\"filters_for_layer_{layer_name}.png\", stitched_filters)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af07bc41",
      "metadata": {
        "id": "af07bc41"
      },
      "source": [
        "### Visualizing heatmaps of class activation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e9d6790",
      "metadata": {
        "id": "2e9d6790"
      },
      "source": [
        "**Loading the Xception network with pretrained weights**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93ca70e7",
      "metadata": {
        "id": "93ca70e7"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.xception.Xception(weights=\"imagenet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e61bb1f",
      "metadata": {
        "id": "0e61bb1f"
      },
      "source": [
        "**Preprocessing an input image for Xception**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40d2c4bf",
      "metadata": {
        "id": "40d2c4bf"
      },
      "outputs": [],
      "source": [
        "img_path = keras.utils.get_file(\n",
        "    fname=\"elephant.jpg\",\n",
        "    origin=\"https://img-datasets.s3.amazonaws.com/elephant.jpg\")\n",
        "\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = keras.utils.load_img(img_path, target_size=target_size)\n",
        "    array = keras.utils.img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    array = keras.applications.xception.preprocess_input(array)\n",
        "    return array\n",
        "\n",
        "img_array = get_img_array(img_path, target_size=(299, 299))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1266351",
      "metadata": {
        "id": "e1266351"
      },
      "outputs": [],
      "source": [
        "preds = model.predict(img_array)\n",
        "print(keras.applications.xception.decode_predictions(preds, top=3)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd21800",
      "metadata": {
        "id": "ffd21800"
      },
      "outputs": [],
      "source": [
        "np.argmax(preds[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "003c917f",
      "metadata": {
        "id": "003c917f"
      },
      "source": [
        "**Setting up a model that returns the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "240973a3",
      "metadata": {
        "id": "240973a3"
      },
      "outputs": [],
      "source": [
        "last_conv_layer_name = \"block14_sepconv2_act\"\n",
        "classifier_layer_names = [\n",
        "    \"avg_pool\",\n",
        "    \"predictions\",\n",
        "]\n",
        "last_conv_layer = model.get_layer(last_conv_layer_name)\n",
        "last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a788eb5a",
      "metadata": {
        "id": "a788eb5a"
      },
      "source": [
        "**Reapplying the classifier on top of the last convolutional output**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39de30e0",
      "metadata": {
        "id": "39de30e0"
      },
      "outputs": [],
      "source": [
        "classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
        "x = classifier_input\n",
        "for layer_name in classifier_layer_names:\n",
        "    x = model.get_layer(layer_name)(x)\n",
        "classifier_model = keras.Model(classifier_input, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb115139",
      "metadata": {
        "id": "bb115139"
      },
      "source": [
        "**Retrieving the gradients of the top predicted class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2dc5e25",
      "metadata": {
        "id": "a2dc5e25"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "    last_conv_layer_output = last_conv_layer_model(img_array)\n",
        "    tape.watch(last_conv_layer_output)\n",
        "    preds = classifier_model(last_conv_layer_output)\n",
        "    top_pred_index = tf.argmax(preds[0])\n",
        "    top_class_channel = preds[:, top_pred_index]\n",
        "\n",
        "grads = tape.gradient(top_class_channel, last_conv_layer_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce551a5c",
      "metadata": {
        "id": "ce551a5c"
      },
      "source": [
        "**Gradient pooling and channel-importance weighting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c13cf318",
      "metadata": {
        "id": "c13cf318"
      },
      "outputs": [],
      "source": [
        "pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2)).numpy()\n",
        "last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
        "for i in range(pooled_grads.shape[-1]):\n",
        "    last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
        "heatmap = np.mean(last_conv_layer_output, axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f2ea250",
      "metadata": {
        "id": "0f2ea250"
      },
      "source": [
        "**Heatmap post-processing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec18d978",
      "metadata": {
        "id": "ec18d978"
      },
      "outputs": [],
      "source": [
        "heatmap = np.maximum(heatmap, 0)\n",
        "heatmap /= np.max(heatmap)\n",
        "plt.matshow(heatmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0328ed",
      "metadata": {
        "id": "1d0328ed"
      },
      "source": [
        "**Superimposing the heatmap on the original picture**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5acd4d9",
      "metadata": {
        "id": "b5acd4d9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.cm as cm\n",
        "\n",
        "img = keras.utils.load_img(img_path)\n",
        "img = keras.utils.img_to_array(img)\n",
        "\n",
        "heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "jet = cm.get_cmap(\"jet\")\n",
        "jet_colors = jet(np.arange(256))[:, :3]\n",
        "jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "jet_heatmap = keras.utils.array_to_img(jet_heatmap)\n",
        "jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "jet_heatmap = keras.utils.img_to_array(jet_heatmap)\n",
        "\n",
        "superimposed_img = jet_heatmap * 0.4 + img\n",
        "superimposed_img = keras.utils.array_to_img(superimposed_img)\n",
        "\n",
        "save_path = \"elephant_cam.jpg\"\n",
        "superimposed_img.save(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c6a64bd",
      "metadata": {
        "id": "2c6a64bd"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a35b6ed1",
      "metadata": {
        "id": "a35b6ed1"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0fa8b70",
      "metadata": {
        "id": "c0fa8b70"
      },
      "source": [
        "# Deep learning for timeseries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63152d5d",
      "metadata": {
        "id": "63152d5d"
      },
      "source": [
        "## Different kinds of timeseries tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "893b2d2b",
      "metadata": {
        "id": "893b2d2b"
      },
      "source": [
        "## A temperature-forecasting example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61763d8a",
      "metadata": {
        "id": "61763d8a"
      },
      "outputs": [],
      "source": [
        "!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip\n",
        "!unzip jena_climate_2009_2016.csv.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d9b6778",
      "metadata": {
        "id": "5d9b6778"
      },
      "source": [
        "**Inspecting the data of the Jena weather dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d9bc20c",
      "metadata": {
        "id": "2d9bc20c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "fname = os.path.join(\"jena_climate_2009_2016.csv\")\n",
        "\n",
        "with open(fname) as f:\n",
        "    data = f.read()\n",
        "\n",
        "lines = data.split(\"\\n\")\n",
        "header = lines[0].split(\",\")\n",
        "lines = lines[1:]\n",
        "print(header)\n",
        "print(len(lines))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f04160a5",
      "metadata": {
        "id": "f04160a5"
      },
      "source": [
        "**Parsing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "892c66e5",
      "metadata": {
        "id": "892c66e5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "temperature = np.zeros((len(lines),))\n",
        "raw_data = np.zeros((len(lines), len(header) - 1))\n",
        "for i, line in enumerate(lines):\n",
        "    values = [float(x) for x in line.split(\",\")[1:]]\n",
        "    temperature[i] = values[1]\n",
        "    raw_data[i, :] = values[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc90f8a",
      "metadata": {
        "id": "6bc90f8a"
      },
      "source": [
        "**Plotting the temperature timeseries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9abd169b",
      "metadata": {
        "id": "9abd169b"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "plt.plot(range(len(temperature)), temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5b4e170",
      "metadata": {
        "id": "a5b4e170"
      },
      "source": [
        "**Plotting the first 10 days of the temperature timeseries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2912d6dd",
      "metadata": {
        "id": "2912d6dd"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(1440), temperature[:1440])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "199a7db4",
      "metadata": {
        "id": "199a7db4"
      },
      "source": [
        "**Computing the number of samples we'll use for each data split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "769530ee",
      "metadata": {
        "id": "769530ee"
      },
      "outputs": [],
      "source": [
        "num_train_samples = int(0.5 * len(raw_data))\n",
        "num_val_samples = int(0.25 * len(raw_data))\n",
        "num_test_samples = len(raw_data) - num_train_samples - num_val_samples\n",
        "print(\"num_train_samples:\", num_train_samples)\n",
        "print(\"num_val_samples:\", num_val_samples)\n",
        "print(\"num_test_samples:\", num_test_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111a1e33",
      "metadata": {
        "id": "111a1e33"
      },
      "source": [
        "### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b11a7f0",
      "metadata": {
        "id": "2b11a7f0"
      },
      "source": [
        "**Normalizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f05dfffc",
      "metadata": {
        "id": "f05dfffc"
      },
      "outputs": [],
      "source": [
        "mean = raw_data[:num_train_samples].mean(axis=0)\n",
        "raw_data -= mean\n",
        "std = raw_data[:num_train_samples].std(axis=0)\n",
        "raw_data /= std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45691cb1",
      "metadata": {
        "id": "45691cb1"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "int_sequence = np.arange(10)\n",
        "dummy_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    data=int_sequence[:-3],\n",
        "    targets=int_sequence[3:],\n",
        "    sequence_length=3,\n",
        "    batch_size=2,\n",
        ")\n",
        "\n",
        "for inputs, targets in dummy_dataset:\n",
        "    for i in range(inputs.shape[0]):\n",
        "        print([int(x) for x in inputs[i]], int(targets[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "896d2123",
      "metadata": {
        "id": "896d2123"
      },
      "source": [
        "**Instantiating datasets for training, validation, and testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b43096ff",
      "metadata": {
        "id": "b43096ff"
      },
      "outputs": [],
      "source": [
        "sampling_rate = 6\n",
        "sequence_length = 120\n",
        "delay = sampling_rate * (sequence_length + 24 - 1)\n",
        "batch_size = 256\n",
        "\n",
        "train_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=0,\n",
        "    end_index=num_train_samples)\n",
        "\n",
        "val_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples,\n",
        "    end_index=num_train_samples + num_val_samples)\n",
        "\n",
        "test_dataset = keras.utils.timeseries_dataset_from_array(\n",
        "    raw_data[:-delay],\n",
        "    targets=temperature[delay:],\n",
        "    sampling_rate=sampling_rate,\n",
        "    sequence_length=sequence_length,\n",
        "    shuffle=True,\n",
        "    batch_size=batch_size,\n",
        "    start_index=num_train_samples + num_val_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23080591",
      "metadata": {
        "id": "23080591"
      },
      "source": [
        "**Inspecting the output of one of our datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b708cd",
      "metadata": {
        "id": "57b708cd"
      },
      "outputs": [],
      "source": [
        "for samples, targets in train_dataset:\n",
        "    print(\"samples shape:\", samples.shape)\n",
        "    print(\"targets shape:\", targets.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01d36d44",
      "metadata": {
        "id": "01d36d44"
      },
      "source": [
        "### A common-sense, non-machine-learning baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0409e10f",
      "metadata": {
        "id": "0409e10f"
      },
      "source": [
        "**Computing the common-sense baseline MAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac3fc07",
      "metadata": {
        "id": "0ac3fc07"
      },
      "outputs": [],
      "source": [
        "def evaluate_naive_method(dataset):\n",
        "    total_abs_err = 0.\n",
        "    samples_seen = 0\n",
        "    for samples, targets in dataset:\n",
        "        preds = samples[:, -1, 1] * std[1] + mean[1]\n",
        "        total_abs_err += np.sum(np.abs(preds - targets))\n",
        "        samples_seen += samples.shape[0]\n",
        "    return total_abs_err / samples_seen\n",
        "\n",
        "print(f\"Validation MAE: {evaluate_naive_method(val_dataset):.2f}\")\n",
        "print(f\"Test MAE: {evaluate_naive_method(test_dataset):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8591a5f",
      "metadata": {
        "id": "b8591a5f"
      },
      "source": [
        "### Let's try a basic machine-learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8e5ec65",
      "metadata": {
        "id": "b8e5ec65"
      },
      "source": [
        "**Training and evaluating a densely connected model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d5f7489",
      "metadata": {
        "id": "0d5f7489"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_dense.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_dense.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c34680fb",
      "metadata": {
        "id": "c34680fb"
      },
      "source": [
        "**Plotting results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "329ff381",
      "metadata": {
        "id": "329ff381"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "loss = history.history[\"mae\"]\n",
        "val_loss = history.history[\"val_mae\"]\n",
        "epochs = range(1, len(loss) + 1)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training MAE\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation MAE\")\n",
        "plt.title(\"Training and validation MAE\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d32bd26d",
      "metadata": {
        "id": "d32bd26d"
      },
      "source": [
        "### Let's try a 1D convolutional model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71fea6a8",
      "metadata": {
        "id": "71fea6a8"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Conv1D(8, 24, activation=\"relu\")(inputs)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 12, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(2)(x)\n",
        "x = layers.Conv1D(8, 6, activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_conv.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_conv.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac5b9a1",
      "metadata": {
        "id": "cac5b9a1"
      },
      "source": [
        "### A first recurrent baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a47a726",
      "metadata": {
        "id": "3a47a726"
      },
      "source": [
        "**A simple LSTM-based model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30d64dbc",
      "metadata": {
        "id": "30d64dbc"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(16)(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"jena_lstm.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e813105e",
      "metadata": {
        "id": "e813105e"
      },
      "source": [
        "## Understanding recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b5cb884",
      "metadata": {
        "id": "8b5cb884"
      },
      "source": [
        "**NumPy implementation of a simple RNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "038d03fe",
      "metadata": {
        "id": "038d03fe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "timesteps = 100\n",
        "input_features = 32\n",
        "output_features = 64\n",
        "inputs = np.random.random((timesteps, input_features))\n",
        "state_t = np.zeros((output_features,))\n",
        "W = np.random.random((output_features, input_features))\n",
        "U = np.random.random((output_features, output_features))\n",
        "b = np.random.random((output_features,))\n",
        "successive_outputs = []\n",
        "for input_t in inputs:\n",
        "    output_t = np.tanh(np.dot(W, input_t) + np.dot(U, state_t) + b)\n",
        "    successive_outputs.append(output_t)\n",
        "    state_t = output_t\n",
        "final_output_sequence = np.stack(successive_outputs, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b816bd",
      "metadata": {
        "id": "84b816bd"
      },
      "source": [
        "### A recurrent layer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c3cf598",
      "metadata": {
        "id": "4c3cf598"
      },
      "source": [
        "**An RNN layer that can process sequences of any length**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46d971f",
      "metadata": {
        "id": "b46d971f"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "inputs = keras.Input(shape=(None, num_features))\n",
        "outputs = layers.SimpleRNN(16)(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb193ac",
      "metadata": {
        "id": "3fb193ac"
      },
      "source": [
        "**An RNN layer that returns only its last output step**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dcf8842",
      "metadata": {
        "id": "9dcf8842"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences=False)(inputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4350f4b9",
      "metadata": {
        "id": "4350f4b9"
      },
      "source": [
        "**An RNN layer that returns its full output sequence**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "538f372e",
      "metadata": {
        "id": "538f372e"
      },
      "outputs": [],
      "source": [
        "num_features = 14\n",
        "steps = 120\n",
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "outputs = layers.SimpleRNN(16, return_sequences=True)(inputs)\n",
        "print(outputs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f48cf5",
      "metadata": {
        "id": "d7f48cf5"
      },
      "source": [
        "**Stacking RNN layers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21febbf5",
      "metadata": {
        "id": "21febbf5"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(steps, num_features))\n",
        "x = layers.SimpleRNN(16, return_sequences=True)(inputs)\n",
        "x = layers.SimpleRNN(16, return_sequences=True)(x)\n",
        "outputs = layers.SimpleRNN(16)(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7fde4db",
      "metadata": {
        "id": "c7fde4db"
      },
      "source": [
        "## Advanced use of recurrent neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7af3c9df",
      "metadata": {
        "id": "7af3c9df"
      },
      "source": [
        "### Using recurrent dropout to fight overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e5140fb",
      "metadata": {
        "id": "2e5140fb"
      },
      "source": [
        "**Training and evaluating a dropout-regularized LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdd0147b",
      "metadata": {
        "id": "bdd0147b"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_lstm_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b935ed",
      "metadata": {
        "id": "b6b935ed"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, num_features))\n",
        "x = layers.LSTM(32, recurrent_dropout=0.2, unroll=True)(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1dc7ee9",
      "metadata": {
        "id": "e1dc7ee9"
      },
      "source": [
        "### Stacking recurrent layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf803592",
      "metadata": {
        "id": "bf803592"
      },
      "source": [
        "**Training and evaluating a dropout-regularized, stacked GRU model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efde6bb8",
      "metadata": {
        "id": "efde6bb8"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)\n",
        "x = layers.GRU(32, recurrent_dropout=0.5)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"jena_stacked_gru_dropout.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=50,\n",
        "                    validation_data=val_dataset,\n",
        "                    callbacks=callbacks)\n",
        "model = keras.models.load_model(\"jena_stacked_gru_dropout.keras\")\n",
        "print(f\"Test MAE: {model.evaluate(test_dataset)[1]:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9b6ec0",
      "metadata": {
        "id": "2f9b6ec0"
      },
      "source": [
        "### Using bidirectional RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "983a755b",
      "metadata": {
        "id": "983a755b"
      },
      "source": [
        "**Training and evaluating a bidirectional LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c957f7",
      "metadata": {
        "id": "c6c957f7"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))\n",
        "x = layers.Bidirectional(layers.LSTM(16))(inputs)\n",
        "outputs = layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[\"mae\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cece12f7",
      "metadata": {
        "id": "cece12f7"
      },
      "source": [
        "### Going even further"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47ee0af4",
      "metadata": {
        "id": "47ee0af4"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64ac424",
      "metadata": {
        "id": "c64ac424"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4542ce0",
      "metadata": {
        "id": "f4542ce0"
      },
      "source": [
        "# Deep learning for text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7c24be",
      "metadata": {
        "id": "ef7c24be"
      },
      "source": [
        "## Natural-language processing: The bird's eye view"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e48882",
      "metadata": {
        "id": "61e48882"
      },
      "source": [
        "## Preparing text data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2c60a1e",
      "metadata": {
        "id": "a2c60a1e"
      },
      "source": [
        "### Text standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf95821f",
      "metadata": {
        "id": "cf95821f"
      },
      "source": [
        "### Text splitting (tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993e4b86",
      "metadata": {
        "id": "993e4b86"
      },
      "source": [
        "### Vocabulary indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c433696c",
      "metadata": {
        "id": "c433696c"
      },
      "source": [
        "### Using the TextVectorization layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39449f26",
      "metadata": {
        "id": "39449f26"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "    def standardize(self, text):\n",
        "        text = text.lower()\n",
        "        return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        text = self.standardize(text)\n",
        "        return text.split()\n",
        "\n",
        "    def make_vocabulary(self, dataset):\n",
        "        self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "        for text in dataset:\n",
        "            text = self.standardize(text)\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if token not in self.vocabulary:\n",
        "                    self.vocabulary[token] = len(self.vocabulary)\n",
        "        self.inverse_vocabulary = dict(\n",
        "            (v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "    def encode(self, text):\n",
        "        text = self.standardize(text)\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "    def decode(self, int_sequence):\n",
        "        return \" \".join(\n",
        "            self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3026700",
      "metadata": {
        "id": "b3026700"
      },
      "outputs": [],
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3cf1ab6",
      "metadata": {
        "id": "b3cf1ab6"
      },
      "outputs": [],
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e40aedf",
      "metadata": {
        "id": "3e40aedf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b07596e",
      "metadata": {
        "id": "8b07596e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "    lowercase_string = tf.strings.lower(string_tensor)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "    return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        "    standardize=custom_standardization_fn,\n",
        "    split=custom_split_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be27407e",
      "metadata": {
        "id": "be27407e"
      },
      "outputs": [],
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c4a079e",
      "metadata": {
        "id": "8c4a079e"
      },
      "source": [
        "**Displaying the vocabulary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25034047",
      "metadata": {
        "id": "25034047"
      },
      "outputs": [],
      "source": [
        "text_vectorization.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7942fae",
      "metadata": {
        "id": "c7942fae"
      },
      "outputs": [],
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30730e15",
      "metadata": {
        "id": "30730e15"
      },
      "outputs": [],
      "source": [
        "inverse_vocab = dict(enumerate(vocabulary))\n",
        "decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
        "print(decoded_sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8b678bf",
      "metadata": {
        "id": "a8b678bf"
      },
      "source": [
        "## Two approaches for representing groups of words: Sets and sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3f519c9",
      "metadata": {
        "id": "a3f519c9"
      },
      "source": [
        "### Preparing the IMDB movie reviews data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f069c53f",
      "metadata": {
        "id": "f069c53f"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df00feaa",
      "metadata": {
        "id": "df00feaa"
      },
      "outputs": [],
      "source": [
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0da07795",
      "metadata": {
        "id": "0da07795"
      },
      "outputs": [],
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25d9c1e0",
      "metadata": {
        "id": "25d9c1e0"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5272b005",
      "metadata": {
        "id": "5272b005"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76cda886",
      "metadata": {
        "id": "76cda886"
      },
      "source": [
        "**Displaying the shapes and dtypes of the first batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc4a54bc",
      "metadata": {
        "id": "fc4a54bc"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a5d34f8",
      "metadata": {
        "id": "4a5d34f8"
      },
      "source": [
        "### Processing words as a set: The bag-of-words approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "181b35b6",
      "metadata": {
        "id": "181b35b6"
      },
      "source": [
        "#### Single words (unigrams) with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "573ec583",
      "metadata": {
        "id": "573ec583"
      },
      "source": [
        "**Preprocessing our datasets with a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe5fae4",
      "metadata": {
        "id": "cfe5fae4"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4df94c98",
      "metadata": {
        "id": "4df94c98"
      },
      "source": [
        "**Inspecting the output of our binary unigram dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22183b0d",
      "metadata": {
        "id": "22183b0d"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4fbd1fe",
      "metadata": {
        "id": "f4fbd1fe"
      },
      "source": [
        "**Our model-building utility**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5753c6ae",
      "metadata": {
        "id": "5753c6ae"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e29f286",
      "metadata": {
        "id": "7e29f286"
      },
      "source": [
        "**Training and testing the binary unigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1823ae38",
      "metadata": {
        "id": "1823ae38"
      },
      "outputs": [],
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa43d3f7",
      "metadata": {
        "id": "aa43d3f7"
      },
      "source": [
        "#### Bigrams with binary encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d32614d",
      "metadata": {
        "id": "3d32614d"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50ee989",
      "metadata": {
        "id": "d50ee989"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b8c7e19",
      "metadata": {
        "id": "3b8c7e19"
      },
      "source": [
        "**Training and testing the binary bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f427859b",
      "metadata": {
        "id": "f427859b"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67519310",
      "metadata": {
        "id": "67519310"
      },
      "source": [
        "#### Bigrams with TF-IDF encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f29b900c",
      "metadata": {
        "id": "f29b900c"
      },
      "source": [
        "**Configuring the `TextVectorization` layer to return token counts**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "148c9f3c",
      "metadata": {
        "id": "148c9f3c"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"count\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab45116",
      "metadata": {
        "id": "eab45116"
      },
      "source": [
        "**Configuring `TextVectorization` to return TF-IDF-weighted outputs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cab7a9e8",
      "metadata": {
        "id": "cab7a9e8"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e194ba86",
      "metadata": {
        "id": "e194ba86"
      },
      "source": [
        "**Training and testing the TF-IDF bigram model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e588edee",
      "metadata": {
        "id": "e588edee"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c941292f",
      "metadata": {
        "id": "c941292f"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a46d6e3",
      "metadata": {
        "id": "9a46d6e3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f83c53",
      "metadata": {
        "id": "f5f83c53"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e887f868",
      "metadata": {
        "id": "e887f868"
      },
      "source": [
        "### Processing words as a sequence: The sequence model approach"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38a87708",
      "metadata": {
        "id": "38a87708"
      },
      "source": [
        "#### A first practical example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e01b51c4",
      "metadata": {
        "id": "e01b51c4"
      },
      "source": [
        "**Downloading the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958c9281",
      "metadata": {
        "id": "958c9281"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeb80f15",
      "metadata": {
        "id": "aeb80f15"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce771d18",
      "metadata": {
        "id": "ce771d18"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f4c8b6",
      "metadata": {
        "id": "15f4c8b6"
      },
      "source": [
        "**Preparing integer sequence datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94228181",
      "metadata": {
        "id": "94228181"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d2e3768",
      "metadata": {
        "id": "7d2e3768"
      },
      "source": [
        "**A sequence model built on one-hot encoded vector sequences**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47b85b58",
      "metadata": {
        "id": "47b85b58"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34cae99f",
      "metadata": {
        "id": "34cae99f"
      },
      "source": [
        "**Training a first basic sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc151c25",
      "metadata": {
        "id": "bc151c25"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe0b905e",
      "metadata": {
        "id": "fe0b905e"
      },
      "source": [
        "#### Understanding word embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93bd798a",
      "metadata": {
        "id": "93bd798a"
      },
      "source": [
        "#### Learning word embeddings with the Embedding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8708ff",
      "metadata": {
        "id": "4a8708ff"
      },
      "source": [
        "**Instantiating an `Embedding` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e7b27ca",
      "metadata": {
        "id": "2e7b27ca"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "283b9ee9",
      "metadata": {
        "id": "283b9ee9"
      },
      "source": [
        "**Model that uses an `Embedding` layer trained from scratch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13b08fa1",
      "metadata": {
        "id": "13b08fa1"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "526a9c85",
      "metadata": {
        "id": "526a9c85"
      },
      "source": [
        "#### Understanding padding and masking"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b62bd6",
      "metadata": {
        "id": "25b62bd6"
      },
      "source": [
        "**Using an `Embedding` layer with masking enabled**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38b46c9f",
      "metadata": {
        "id": "38b46c9f"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77cd3ce5",
      "metadata": {
        "id": "77cd3ce5"
      },
      "source": [
        "#### Using pretrained word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49a62a6",
      "metadata": {
        "id": "f49a62a6"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83f91604",
      "metadata": {
        "id": "83f91604"
      },
      "source": [
        "**Parsing the GloVe word-embeddings file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65e72d38",
      "metadata": {
        "id": "65e72d38"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "486a4fb2",
      "metadata": {
        "id": "486a4fb2"
      },
      "source": [
        "**Preparing the GloVe word-embeddings matrix**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f436648",
      "metadata": {
        "id": "1f436648"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "351b9338",
      "metadata": {
        "id": "351b9338"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "411fd6ce",
      "metadata": {
        "id": "411fd6ce"
      },
      "source": [
        "**Model that uses a pretrained Embedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88f6ddd7",
      "metadata": {
        "id": "88f6ddd7"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c90c27bc",
      "metadata": {
        "id": "c90c27bc"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d337476",
      "metadata": {
        "id": "0d337476"
      },
      "source": [
        "## The Transformer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc8c619c",
      "metadata": {
        "id": "cc8c619c"
      },
      "source": [
        "### Understanding self-attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f071289",
      "metadata": {
        "id": "3f071289"
      },
      "source": [
        "#### Generalized self-attention: the query-key-value model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a61f86b7",
      "metadata": {
        "id": "a61f86b7"
      },
      "source": [
        "### Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee3ba09",
      "metadata": {
        "id": "2ee3ba09"
      },
      "source": [
        "### The Transformer encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f58e153",
      "metadata": {
        "id": "9f58e153"
      },
      "source": [
        "**Getting the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cddb5e16",
      "metadata": {
        "id": "cddb5e16"
      },
      "outputs": [],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c431582",
      "metadata": {
        "id": "8c431582"
      },
      "source": [
        "**Preparing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07f71685",
      "metadata": {
        "id": "07f71685"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.2 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ba37da",
      "metadata": {
        "id": "85ba37da"
      },
      "source": [
        "**Vectorizing the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863b13e3",
      "metadata": {
        "id": "863b13e3"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28b8183d",
      "metadata": {
        "id": "28b8183d"
      },
      "source": [
        "**Transformer encoder implemented as a subclassed `Layer`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b218b7b4",
      "metadata": {
        "id": "b218b7b4"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            mask = mask[:, tf.newaxis, :]\n",
        "        attention_output = self.attention(\n",
        "            inputs, inputs, attention_mask=mask)\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "773a8c3d",
      "metadata": {
        "id": "773a8c3d"
      },
      "source": [
        "**Using the Transformer encoder for text classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c904588d",
      "metadata": {
        "id": "c904588d"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf55f7c9",
      "metadata": {
        "id": "bf55f7c9"
      },
      "source": [
        "**Training and evaluating the Transformer encoder based model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7512e913",
      "metadata": {
        "id": "7512e913"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47d1062",
      "metadata": {
        "id": "d47d1062"
      },
      "source": [
        "#### Using positional encoding to re-inject order information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d0a84a",
      "metadata": {
        "id": "86d0a84a"
      },
      "source": [
        "**Implementing positional embedding as a subclassed layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac487f72",
      "metadata": {
        "id": "ac487f72"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64267237",
      "metadata": {
        "id": "64267237"
      },
      "source": [
        "#### Putting it all together: A text-classification Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7db3661",
      "metadata": {
        "id": "a7db3661"
      },
      "source": [
        "**Combining the Transformer encoder with positional embedding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31860c59",
      "metadata": {
        "id": "31860c59"
      },
      "outputs": [],
      "source": [
        "vocab_size = 20000\n",
        "sequence_length = 600\n",
        "embed_dim = 256\n",
        "num_heads = 2\n",
        "dense_dim = 32\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
        "model = keras.models.load_model(\n",
        "    \"full_transformer_encoder.keras\",\n",
        "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
        "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57714bc2",
      "metadata": {
        "id": "57714bc2"
      },
      "source": [
        "### When to use sequence models over bag-of-words models?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b56fdcf",
      "metadata": {
        "id": "3b56fdcf"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e05396e",
      "metadata": {
        "id": "3e05396e"
      },
      "source": [
        "## Beyond text classification: Sequence-to-sequence learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d33855b2",
      "metadata": {
        "id": "d33855b2"
      },
      "source": [
        "### A machine translation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e92212",
      "metadata": {
        "id": "d1e92212"
      },
      "outputs": [],
      "source": [
        "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
        "!unzip -q spa-eng.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f38388a6",
      "metadata": {
        "id": "f38388a6"
      },
      "outputs": [],
      "source": [
        "text_file = \"spa-eng/spa.txt\"\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    english, spanish = line.split(\"\\t\")\n",
        "    spanish = \"[start] \" + spanish + \" [end]\"\n",
        "    text_pairs.append((english, spanish))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "860a7967",
      "metadata": {
        "id": "860a7967"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "print(random.choice(text_pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63f377ad",
      "metadata": {
        "id": "63f377ad"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "279c94bb",
      "metadata": {
        "id": "279c94bb"
      },
      "source": [
        "**Vectorizing the English and Spanish text pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "682bcd28",
      "metadata": {
        "id": "682bcd28"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "strip_chars = string.punctuation + \"¿\"\n",
        "strip_chars = strip_chars.replace(\"[\", \"\")\n",
        "strip_chars = strip_chars.replace(\"]\", \"\")\n",
        "\n",
        "def custom_standardization(input_string):\n",
        "    lowercase = tf.strings.lower(input_string)\n",
        "    return tf.strings.regex_replace(\n",
        "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
        "\n",
        "vocab_size = 15000\n",
        "sequence_length = 20\n",
        "\n",
        "source_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "target_vectorization = layers.TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    standardize=custom_standardization,\n",
        ")\n",
        "train_english_texts = [pair[0] for pair in train_pairs]\n",
        "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
        "source_vectorization.adapt(train_english_texts)\n",
        "target_vectorization.adapt(train_spanish_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0009ba96",
      "metadata": {
        "id": "0009ba96"
      },
      "source": [
        "**Preparing datasets for the translation task**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93232d79",
      "metadata": {
        "id": "93232d79"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "def format_dataset(eng, spa):\n",
        "    eng = source_vectorization(eng)\n",
        "    spa = target_vectorization(spa)\n",
        "    return ({\n",
        "        \"english\": eng,\n",
        "        \"spanish\": spa[:, :-1],\n",
        "    }, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset, num_parallel_calls=4)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49cec12",
      "metadata": {
        "id": "f49cec12"
      },
      "outputs": [],
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
        "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0639bc7",
      "metadata": {
        "id": "e0639bc7"
      },
      "source": [
        "### Sequence-to-sequence learning with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc49f1e",
      "metadata": {
        "id": "cdc49f1e"
      },
      "source": [
        "**GRU-based encoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "542ff90a",
      "metadata": {
        "id": "542ff90a"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "embed_dim = 256\n",
        "latent_dim = 1024\n",
        "\n",
        "source = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(source)\n",
        "encoded_source = layers.Bidirectional(\n",
        "    layers.GRU(latent_dim), merge_mode=\"sum\")(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11353124",
      "metadata": {
        "id": "11353124"
      },
      "source": [
        "**GRU-based decoder and the end-to-end model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07d53f66",
      "metadata": {
        "id": "07d53f66"
      },
      "outputs": [],
      "source": [
        "past_target = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = layers.Embedding(vocab_size, embed_dim, mask_zero=True)(past_target)\n",
        "decoder_gru = layers.GRU(latent_dim, return_sequences=True)\n",
        "x = decoder_gru(x, initial_state=encoded_source)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "target_next_step = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "seq2seq_rnn = keras.Model([source, past_target], target_next_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6860d2",
      "metadata": {
        "id": "3c6860d2"
      },
      "source": [
        "**Training our recurrent sequence-to-sequence model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01bde3d2",
      "metadata": {
        "id": "01bde3d2"
      },
      "outputs": [],
      "source": [
        "seq2seq_rnn.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "seq2seq_rnn.fit(train_ds, epochs=15, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "484c234d",
      "metadata": {
        "id": "484c234d"
      },
      "source": [
        "**Translating new sentences with our RNN encoder and decoder**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364cd303",
      "metadata": {
        "id": "364cd303"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization([decoded_sentence])\n",
        "        next_token_predictions = seq2seq_rnn.predict(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(next_token_predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9927aa16",
      "metadata": {
        "id": "9927aa16"
      },
      "source": [
        "### Sequence-to-sequence learning with Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a3579c8",
      "metadata": {
        "id": "6a3579c8"
      },
      "source": [
        "#### The Transformer decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d9d322d",
      "metadata": {
        "id": "4d9d322d"
      },
      "source": [
        "**The `TransformerDecoder`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a5a15c",
      "metadata": {
        "id": "35a5a15c"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8c9e6c4",
      "metadata": {
        "id": "a8c9e6c4"
      },
      "source": [
        "#### Putting it all together: A Transformer for machine translation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df3dba02",
      "metadata": {
        "id": "df3dba02"
      },
      "source": [
        "**PositionalEmbedding layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e795ab",
      "metadata": {
        "id": "86e795ab"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fcb635",
      "metadata": {
        "id": "84fcb635"
      },
      "source": [
        "**End-to-end Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55b5e1c1",
      "metadata": {
        "id": "55b5e1c1"
      },
      "outputs": [],
      "source": [
        "embed_dim = 256\n",
        "dense_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52ea0875",
      "metadata": {
        "id": "52ea0875"
      },
      "source": [
        "**Training the sequence-to-sequence Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5b0d0ce",
      "metadata": {
        "id": "a5b0d0ce"
      },
      "outputs": [],
      "source": [
        "transformer.compile(\n",
        "    optimizer=\"rmsprop\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"])\n",
        "transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de0845a0",
      "metadata": {
        "id": "de0845a0"
      },
      "source": [
        "**Translating new sentences with our Transformer model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74342243",
      "metadata": {
        "id": "74342243"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "spa_vocab = target_vectorization.get_vocabulary()\n",
        "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
        "max_decoded_sentence_length = 20\n",
        "\n",
        "def decode_sequence(input_sentence):\n",
        "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
        "    decoded_sentence = \"[start]\"\n",
        "    for i in range(max_decoded_sentence_length):\n",
        "        tokenized_target_sentence = target_vectorization(\n",
        "            [decoded_sentence])[:, :-1]\n",
        "        predictions = transformer(\n",
        "            [tokenized_input_sentence, tokenized_target_sentence])\n",
        "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
        "        sampled_token = spa_index_lookup[sampled_token_index]\n",
        "        decoded_sentence += \" \" + sampled_token\n",
        "        if sampled_token == \"[end]\":\n",
        "            break\n",
        "    return decoded_sentence\n",
        "\n",
        "test_eng_texts = [pair[0] for pair in test_pairs]\n",
        "for _ in range(20):\n",
        "    input_sentence = random.choice(test_eng_texts)\n",
        "    print(\"-\")\n",
        "    print(input_sentence)\n",
        "    print(decode_sequence(input_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19952266",
      "metadata": {
        "id": "19952266"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b6257d0",
      "metadata": {
        "id": "1b6257d0"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95ae58c7",
      "metadata": {
        "id": "95ae58c7"
      },
      "source": [
        "# Generative deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959bfd1d",
      "metadata": {
        "id": "959bfd1d"
      },
      "source": [
        "## Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea20af8",
      "metadata": {
        "id": "9ea20af8"
      },
      "source": [
        "### A brief history of generative deep learning for sequence generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79cce48b",
      "metadata": {
        "id": "79cce48b"
      },
      "source": [
        "### How do you generate sequence data?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48a9c814",
      "metadata": {
        "id": "48a9c814"
      },
      "source": [
        "### The importance of the sampling strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d43b9110",
      "metadata": {
        "id": "d43b9110"
      },
      "source": [
        "**Reweighting a probability distribution to a different temperature**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "657bb343",
      "metadata": {
        "id": "657bb343"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def reweight_distribution(original_distribution, temperature=0.5):\n",
        "    distribution = np.log(original_distribution) / temperature\n",
        "    distribution = np.exp(distribution)\n",
        "    return distribution / np.sum(distribution)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10b96c26",
      "metadata": {
        "id": "10b96c26"
      },
      "source": [
        "### Implementing text generation with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1deea90",
      "metadata": {
        "id": "d1deea90"
      },
      "source": [
        "#### Preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0707e072",
      "metadata": {
        "id": "0707e072"
      },
      "source": [
        "**Downloading and uncompressing the IMDB movie reviews dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36898a3c",
      "metadata": {
        "id": "36898a3c"
      },
      "outputs": [],
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc253723",
      "metadata": {
        "id": "cc253723"
      },
      "source": [
        "**Creating a dataset from text files (one file = one sample)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1770c4a",
      "metadata": {
        "id": "d1770c4a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "dataset = keras.utils.text_dataset_from_directory(\n",
        "    directory=\"aclImdb\", label_mode=None, batch_size=256)\n",
        "dataset = dataset.map(lambda x: tf.strings.regex_replace(x, \"<br />\", \" \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc217848",
      "metadata": {
        "id": "dc217848"
      },
      "source": [
        "**Preparing a `TextVectorization` layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de6db429",
      "metadata": {
        "id": "de6db429"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "sequence_length = 100\n",
        "vocab_size = 15000\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length,\n",
        ")\n",
        "text_vectorization.adapt(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37407d40",
      "metadata": {
        "id": "37407d40"
      },
      "source": [
        "**Setting up a language modeling dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "890e9393",
      "metadata": {
        "id": "890e9393"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_dataset(text_batch):\n",
        "    vectorized_sequences = text_vectorization(text_batch)\n",
        "    x = vectorized_sequences[:, :-1]\n",
        "    y = vectorized_sequences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "lm_dataset = dataset.map(prepare_lm_dataset, num_parallel_calls=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6a4d158",
      "metadata": {
        "id": "a6a4d158"
      },
      "source": [
        "#### A Transformer-based sequence-to-sequence model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df0f619e",
      "metadata": {
        "id": "df0f619e"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=input_dim, output_dim=output_dim)\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=output_dim)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(PositionalEmbedding, self).get_config()\n",
        "        config.update({\n",
        "            \"output_dim\": self.output_dim,\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"input_dim\": self.input_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "          num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
        "             layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(TransformerDecoder, self).get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1),\n",
        "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
        "        return tf.tile(mask, mult)\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(\n",
        "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "        else:\n",
        "            padding_mask = mask\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs,\n",
        "            value=inputs,\n",
        "            key=inputs,\n",
        "            attention_mask=causal_mask)\n",
        "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=attention_output_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        attention_output_2 = self.layernorm_2(\n",
        "            attention_output_1 + attention_output_2)\n",
        "        proj_output = self.dense_proj(attention_output_2)\n",
        "        return self.layernorm_3(attention_output_2 + proj_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc37d40f",
      "metadata": {
        "id": "dc37d40f"
      },
      "source": [
        "**A simple Transformer-based language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066007bb",
      "metadata": {
        "id": "066007bb"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 2\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cae6779",
      "metadata": {
        "id": "2cae6779"
      },
      "source": [
        "### A text-generation callback with variable-temperature sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "188e352f",
      "metadata": {
        "id": "188e352f"
      },
      "source": [
        "**The text-generation callback**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9922511",
      "metadata": {
        "id": "a9922511"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "tokens_index = dict(enumerate(text_vectorization.get_vocabulary()))\n",
        "\n",
        "def sample_next(predictions, temperature=1.0):\n",
        "    predictions = np.asarray(predictions).astype(\"float64\")\n",
        "    predictions = np.log(predictions) / temperature\n",
        "    exp_preds = np.exp(predictions)\n",
        "    predictions = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, predictions, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    def __init__(self,\n",
        "                 prompt,\n",
        "                 generate_length,\n",
        "                 model_input_length,\n",
        "                 temperatures=(1.,),\n",
        "                 print_freq=1):\n",
        "        self.prompt = prompt\n",
        "        self.generate_length = generate_length\n",
        "        self.model_input_length = model_input_length\n",
        "        self.temperatures = temperatures\n",
        "        self.print_freq = print_freq\n",
        "        vectorized_prompt = text_vectorization([prompt])[0].numpy()\n",
        "        self.prompt_length = np.nonzero(vectorized_prompt == 0)[0][0]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        if (epoch + 1) % self.print_freq != 0:\n",
        "            return\n",
        "        for temperature in self.temperatures:\n",
        "            print(\"== Generating with temperature\", temperature)\n",
        "            sentence = self.prompt\n",
        "            for i in range(self.generate_length):\n",
        "                tokenized_sentence = text_vectorization([sentence])\n",
        "                predictions = self.model(tokenized_sentence)\n",
        "                next_token = sample_next(\n",
        "                    predictions[0, self.prompt_length - 1 + i, :]\n",
        "                )\n",
        "                sampled_token = tokens_index[next_token]\n",
        "                sentence += \" \" + sampled_token\n",
        "            print(sentence)\n",
        "\n",
        "prompt = \"This movie\"\n",
        "text_gen_callback = TextGenerator(\n",
        "    prompt,\n",
        "    generate_length=50,\n",
        "    model_input_length=sequence_length,\n",
        "    temperatures=(0.2, 0.5, 0.7, 1., 1.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4622baa6",
      "metadata": {
        "id": "4622baa6"
      },
      "source": [
        "**Fitting the language model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00d648a",
      "metadata": {
        "id": "f00d648a"
      },
      "outputs": [],
      "source": [
        "model.fit(lm_dataset, epochs=200, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2428e24",
      "metadata": {
        "id": "f2428e24"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e4715cf",
      "metadata": {
        "id": "8e4715cf"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf797ab7",
      "metadata": {
        "id": "bf797ab7"
      },
      "source": [
        "## DeepDream"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a0c056",
      "metadata": {
        "id": "74a0c056"
      },
      "source": [
        "### Implementing DeepDream in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e8c73e",
      "metadata": {
        "id": "a1e8c73e"
      },
      "source": [
        "**Fetching the test image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "105def42",
      "metadata": {
        "id": "105def42"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "base_image_path = keras.utils.get_file(\n",
        "    \"coast.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/coast.jpg\")\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(keras.utils.load_img(base_image_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3df56d12",
      "metadata": {
        "id": "3df56d12"
      },
      "source": [
        "**Instantiating a pretrained `InceptionV3` model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b7f641f",
      "metadata": {
        "id": "2b7f641f"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import inception_v3\n",
        "model = inception_v3.InceptionV3(weights=\"imagenet\", include_top=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf7baf45",
      "metadata": {
        "id": "cf7baf45"
      },
      "source": [
        "**Configuring the contribution of each layer to the DeepDream loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b283e38b",
      "metadata": {
        "id": "b283e38b"
      },
      "outputs": [],
      "source": [
        "layer_settings = {\n",
        "    \"mixed4\": 1.0,\n",
        "    \"mixed5\": 1.5,\n",
        "    \"mixed6\": 2.0,\n",
        "    \"mixed7\": 2.5,\n",
        "}\n",
        "outputs_dict = dict(\n",
        "    [\n",
        "        (layer.name, layer.output)\n",
        "        for layer in [model.get_layer(name) for name in layer_settings.keys()]\n",
        "    ]\n",
        ")\n",
        "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0379516",
      "metadata": {
        "id": "e0379516"
      },
      "source": [
        "**The DeepDream loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c04953f",
      "metadata": {
        "id": "5c04953f"
      },
      "outputs": [],
      "source": [
        "def compute_loss(input_image):\n",
        "    features = feature_extractor(input_image)\n",
        "    loss = tf.zeros(shape=())\n",
        "    for name in features.keys():\n",
        "        coeff = layer_settings[name]\n",
        "        activation = features[name]\n",
        "        loss += coeff * tf.reduce_mean(tf.square(activation[:, 2:-2, 2:-2, :]))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3212c403",
      "metadata": {
        "id": "3212c403"
      },
      "source": [
        "**The DeepDream gradient ascent process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7dce455",
      "metadata": {
        "id": "f7dce455"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def gradient_ascent_step(image, learning_rate):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(image)\n",
        "        loss = compute_loss(image)\n",
        "    grads = tape.gradient(loss, image)\n",
        "    grads = tf.math.l2_normalize(grads)\n",
        "    image += learning_rate * grads\n",
        "    return loss, image\n",
        "\n",
        "\n",
        "def gradient_ascent_loop(image, iterations, learning_rate, max_loss=None):\n",
        "    for i in range(iterations):\n",
        "        loss, image = gradient_ascent_step(image, learning_rate)\n",
        "        if max_loss is not None and loss > max_loss:\n",
        "            break\n",
        "        print(f\"... Loss value at step {i}: {loss:.2f}\")\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "406b195b",
      "metadata": {
        "id": "406b195b"
      },
      "outputs": [],
      "source": [
        "step = 20.\n",
        "num_octave = 3\n",
        "octave_scale = 1.4\n",
        "iterations = 30\n",
        "max_loss = 15."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea157228",
      "metadata": {
        "id": "ea157228"
      },
      "source": [
        "**Image processing utilities**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd5662ac",
      "metadata": {
        "id": "cd5662ac"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = keras.utils.load_img(image_path)\n",
        "    img = keras.utils.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_image(img):\n",
        "    img = img.reshape((img.shape[1], img.shape[2], 3))\n",
        "    img /= 2.0\n",
        "    img += 0.5\n",
        "    img *= 255.\n",
        "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c17511",
      "metadata": {
        "id": "09c17511"
      },
      "source": [
        "**Running gradient ascent over multiple successive \"octaves\"**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "258f00ca",
      "metadata": {
        "id": "258f00ca"
      },
      "outputs": [],
      "source": [
        "original_img = preprocess_image(base_image_path)\n",
        "original_shape = original_img.shape[1:3]\n",
        "\n",
        "successive_shapes = [original_shape]\n",
        "for i in range(1, num_octave):\n",
        "    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n",
        "    successive_shapes.append(shape)\n",
        "successive_shapes = successive_shapes[::-1]\n",
        "\n",
        "shrunk_original_img = tf.image.resize(original_img, successive_shapes[0])\n",
        "\n",
        "img = tf.identity(original_img)\n",
        "for i, shape in enumerate(successive_shapes):\n",
        "    print(f\"Processing octave {i} with shape {shape}\")\n",
        "    img = tf.image.resize(img, shape)\n",
        "    img = gradient_ascent_loop(\n",
        "        img, iterations=iterations, learning_rate=step, max_loss=max_loss\n",
        "    )\n",
        "    upscaled_shrunk_original_img = tf.image.resize(shrunk_original_img, shape)\n",
        "    same_size_original = tf.image.resize(original_img, shape)\n",
        "    lost_detail = same_size_original - upscaled_shrunk_original_img\n",
        "    img += lost_detail\n",
        "    shrunk_original_img = tf.image.resize(original_img, shape)\n",
        "\n",
        "keras.utils.save_img(\"dream.png\", deprocess_image(img.numpy()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8dccc1e",
      "metadata": {
        "id": "a8dccc1e"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50dfb0d2",
      "metadata": {
        "id": "50dfb0d2"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c6be03",
      "metadata": {
        "id": "68c6be03"
      },
      "source": [
        "## Neural style transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd50122",
      "metadata": {
        "id": "5bd50122"
      },
      "source": [
        "### The content loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee6fe018",
      "metadata": {
        "id": "ee6fe018"
      },
      "source": [
        "### The style loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53a08c3",
      "metadata": {
        "id": "b53a08c3"
      },
      "source": [
        "### Neural style transfer in Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2237704d",
      "metadata": {
        "id": "2237704d"
      },
      "source": [
        "**Getting the style and content images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "719d537b",
      "metadata": {
        "id": "719d537b"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "base_image_path = keras.utils.get_file(\n",
        "    \"sf.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/sf.jpg\")\n",
        "style_reference_image_path = keras.utils.get_file(\n",
        "    \"starry_night.jpg\", origin=\"https://img-datasets.s3.amazonaws.com/starry_night.jpg\")\n",
        "\n",
        "original_width, original_height = keras.utils.load_img(base_image_path).size\n",
        "img_height = 400\n",
        "img_width = round(original_width * img_height / original_height)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dad71f",
      "metadata": {
        "id": "d5dad71f"
      },
      "source": [
        "**Auxiliary functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e25305b",
      "metadata": {
        "id": "6e25305b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = keras.utils.load_img(\n",
        "        image_path, target_size=(img_height, img_width))\n",
        "    img = keras.utils.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = keras.applications.vgg19.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def deprocess_image(img):\n",
        "    img = img.reshape((img_height, img_width, 3))\n",
        "    img[:, :, 0] += 103.939\n",
        "    img[:, :, 1] += 116.779\n",
        "    img[:, :, 2] += 123.68\n",
        "    img = img[:, :, ::-1]\n",
        "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ce66db4",
      "metadata": {
        "id": "5ce66db4"
      },
      "source": [
        "**Using a pretrained VGG19 model to create a feature extractor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45e24e91",
      "metadata": {
        "id": "45e24e91"
      },
      "outputs": [],
      "source": [
        "model = keras.applications.vgg19.VGG19(weights=\"imagenet\", include_top=False)\n",
        "\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "feature_extractor = keras.Model(inputs=model.inputs, outputs=outputs_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f95f1f",
      "metadata": {
        "id": "90f95f1f"
      },
      "source": [
        "**Content loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c9a30d",
      "metadata": {
        "id": "97c9a30d"
      },
      "outputs": [],
      "source": [
        "def content_loss(base_img, combination_img):\n",
        "    return tf.reduce_sum(tf.square(combination_img - base_img))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e5fab4e",
      "metadata": {
        "id": "7e5fab4e"
      },
      "source": [
        "**Style loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46950f25",
      "metadata": {
        "id": "46950f25"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(x):\n",
        "    x = tf.transpose(x, (2, 0, 1))\n",
        "    features = tf.reshape(x, (tf.shape(x)[0], -1))\n",
        "    gram = tf.matmul(features, tf.transpose(features))\n",
        "    return gram\n",
        "\n",
        "def style_loss(style_img, combination_img):\n",
        "    S = gram_matrix(style_img)\n",
        "    C = gram_matrix(combination_img)\n",
        "    channels = 3\n",
        "    size = img_height * img_width\n",
        "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7beaac0f",
      "metadata": {
        "id": "7beaac0f"
      },
      "source": [
        "**Total variation loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160840f0",
      "metadata": {
        "id": "160840f0"
      },
      "outputs": [],
      "source": [
        "def total_variation_loss(x):\n",
        "    a = tf.square(\n",
        "        x[:, : img_height - 1, : img_width - 1, :] - x[:, 1:, : img_width - 1, :]\n",
        "    )\n",
        "    b = tf.square(\n",
        "        x[:, : img_height - 1, : img_width - 1, :] - x[:, : img_height - 1, 1:, :]\n",
        "    )\n",
        "    return tf.reduce_sum(tf.pow(a + b, 1.25))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38b88572",
      "metadata": {
        "id": "38b88572"
      },
      "source": [
        "**Defining the final loss that you'll minimize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "481ac516",
      "metadata": {
        "id": "481ac516"
      },
      "outputs": [],
      "source": [
        "style_layer_names = [\n",
        "    \"block1_conv1\",\n",
        "    \"block2_conv1\",\n",
        "    \"block3_conv1\",\n",
        "    \"block4_conv1\",\n",
        "    \"block5_conv1\",\n",
        "]\n",
        "content_layer_name = \"block5_conv2\"\n",
        "total_variation_weight = 1e-6\n",
        "style_weight = 1e-6\n",
        "content_weight = 2.5e-8\n",
        "\n",
        "def compute_loss(combination_image, base_image, style_reference_image):\n",
        "    input_tensor = tf.concat(\n",
        "        [base_image, style_reference_image, combination_image], axis=0\n",
        "    )\n",
        "    features = feature_extractor(input_tensor)\n",
        "    loss = tf.zeros(shape=())\n",
        "    layer_features = features[content_layer_name]\n",
        "    base_image_features = layer_features[0, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    loss = loss + content_weight * content_loss(\n",
        "        base_image_features, combination_features\n",
        "    )\n",
        "    for layer_name in style_layer_names:\n",
        "        layer_features = features[layer_name]\n",
        "        style_reference_features = layer_features[1, :, :, :]\n",
        "        combination_features = layer_features[2, :, :, :]\n",
        "        style_loss_value = style_loss(\n",
        "          style_reference_features, combination_features)\n",
        "        loss += (style_weight / len(style_layer_names)) * style_loss_value\n",
        "\n",
        "    loss += total_variation_weight * total_variation_loss(combination_image)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79364158",
      "metadata": {
        "id": "79364158"
      },
      "source": [
        "**Setting up the gradient-descent process**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "434f0de3",
      "metadata": {
        "id": "434f0de3"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "@tf.function\n",
        "def compute_loss_and_grads(combination_image, base_image, style_reference_image):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = compute_loss(combination_image, base_image, style_reference_image)\n",
        "    grads = tape.gradient(loss, combination_image)\n",
        "    return loss, grads\n",
        "\n",
        "optimizer = keras.optimizers.SGD(\n",
        "    keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=100.0, decay_steps=100, decay_rate=0.96\n",
        "    )\n",
        ")\n",
        "\n",
        "base_image = preprocess_image(base_image_path)\n",
        "style_reference_image = preprocess_image(style_reference_image_path)\n",
        "combination_image = tf.Variable(preprocess_image(base_image_path))\n",
        "\n",
        "iterations = 4000\n",
        "for i in range(1, iterations + 1):\n",
        "    loss, grads = compute_loss_and_grads(\n",
        "        combination_image, base_image, style_reference_image\n",
        "    )\n",
        "    optimizer.apply_gradients([(grads, combination_image)])\n",
        "    if i % 100 == 0:\n",
        "        print(f\"Iteration {i}: loss={loss:.2f}\")\n",
        "        img = deprocess_image(combination_image.numpy())\n",
        "        fname = f\"combination_image_at_iteration_{i}.png\"\n",
        "        keras.utils.save_img(fname, img)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24f14856",
      "metadata": {
        "id": "24f14856"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ac8cdf",
      "metadata": {
        "id": "84ac8cdf"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4043f410",
      "metadata": {
        "id": "4043f410"
      },
      "source": [
        "## Generating images with variational autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffc5a72",
      "metadata": {
        "id": "0ffc5a72"
      },
      "source": [
        "### Sampling from latent spaces of images"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "555356ea",
      "metadata": {
        "id": "555356ea"
      },
      "source": [
        "### Concept vectors for image editing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85099f97",
      "metadata": {
        "id": "85099f97"
      },
      "source": [
        "### Variational autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f541d7",
      "metadata": {
        "id": "e6f541d7"
      },
      "source": [
        "### Implementing a VAE with Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82383c02",
      "metadata": {
        "id": "82383c02"
      },
      "source": [
        "**VAE encoder network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6dcfade",
      "metadata": {
        "id": "f6dcfade"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "latent_dim = 2\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
        "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
        "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var], name=\"encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cb1fb95",
      "metadata": {
        "id": "6cb1fb95"
      },
      "outputs": [],
      "source": [
        "encoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87f1f1a",
      "metadata": {
        "id": "a87f1f1a"
      },
      "source": [
        "**Latent-space-sampling layer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bfd8b5",
      "metadata": {
        "id": "b6bfd8b5"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Sampler(layers.Layer):\n",
        "    def call(self, z_mean, z_log_var):\n",
        "        batch_size = tf.shape(z_mean)[0]\n",
        "        z_size = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.random.normal(shape=(batch_size, z_size))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4eacaf1a",
      "metadata": {
        "id": "4eacaf1a"
      },
      "source": [
        "**VAE decoder network, mapping latent space points to images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3b196dd",
      "metadata": {
        "id": "a3b196dd"
      },
      "outputs": [],
      "source": [
        "latent_inputs = keras.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
        "x = layers.Reshape((7, 7, 64))(x)\n",
        "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
        "decoder_outputs = layers.Conv2D(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
        "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e7ff205",
      "metadata": {
        "id": "6e7ff205"
      },
      "outputs": [],
      "source": [
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18841ba6",
      "metadata": {
        "id": "18841ba6"
      },
      "source": [
        "**VAE model with custom `train_step()`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8600bb45",
      "metadata": {
        "id": "8600bb45"
      },
      "outputs": [],
      "source": [
        "class VAE(keras.Model):\n",
        "    def __init__(self, encoder, decoder, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.sampler = Sampler()\n",
        "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
        "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
        "            name=\"reconstruction_loss\")\n",
        "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.total_loss_tracker,\n",
        "                self.reconstruction_loss_tracker,\n",
        "                self.kl_loss_tracker]\n",
        "\n",
        "    def train_step(self, data):\n",
        "        with tf.GradientTape() as tape:\n",
        "            z_mean, z_log_var = self.encoder(data)\n",
        "            z = self.sampler(z_mean, z_log_var)\n",
        "            reconstruction = decoder(z)\n",
        "            reconstruction_loss = tf.reduce_mean(\n",
        "                tf.reduce_sum(\n",
        "                    keras.losses.binary_crossentropy(data, reconstruction),\n",
        "                    axis=(1, 2)\n",
        "                )\n",
        "            )\n",
        "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
        "            total_loss = reconstruction_loss + tf.reduce_mean(kl_loss)\n",
        "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
        "        self.total_loss_tracker.update_state(total_loss)\n",
        "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
        "        self.kl_loss_tracker.update_state(kl_loss)\n",
        "        return {\n",
        "            \"total_loss\": self.total_loss_tracker.result(),\n",
        "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
        "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e4f41f7",
      "metadata": {
        "id": "9e4f41f7"
      },
      "source": [
        "**Training the VAE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79b94a7c",
      "metadata": {
        "id": "79b94a7c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
        "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
        "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
        "\n",
        "vae = VAE(encoder, decoder)\n",
        "vae.compile(optimizer=keras.optimizers.Adam(), run_eagerly=True)\n",
        "vae.fit(mnist_digits, epochs=30, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27d7456f",
      "metadata": {
        "id": "27d7456f"
      },
      "source": [
        "**Sampling a grid of images from the 2D latent space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "426ab5ca",
      "metadata": {
        "id": "426ab5ca"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n = 30\n",
        "digit_size = 28\n",
        "figure = np.zeros((digit_size * n, digit_size * n))\n",
        "\n",
        "grid_x = np.linspace(-1, 1, n)\n",
        "grid_y = np.linspace(-1, 1, n)[::-1]\n",
        "\n",
        "for i, yi in enumerate(grid_y):\n",
        "    for j, xi in enumerate(grid_x):\n",
        "        z_sample = np.array([[xi, yi]])\n",
        "        x_decoded = vae.decoder.predict(z_sample)\n",
        "        digit = x_decoded[0].reshape(digit_size, digit_size)\n",
        "        figure[\n",
        "            i * digit_size : (i + 1) * digit_size,\n",
        "            j * digit_size : (j + 1) * digit_size,\n",
        "        ] = digit\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "start_range = digit_size // 2\n",
        "end_range = n * digit_size + start_range\n",
        "pixel_range = np.arange(start_range, end_range, digit_size)\n",
        "sample_range_x = np.round(grid_x, 1)\n",
        "sample_range_y = np.round(grid_y, 1)\n",
        "plt.xticks(pixel_range, sample_range_x)\n",
        "plt.yticks(pixel_range, sample_range_y)\n",
        "plt.xlabel(\"z[0]\")\n",
        "plt.ylabel(\"z[1]\")\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(figure, cmap=\"Greys_r\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb651a82",
      "metadata": {
        "id": "eb651a82"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9ebd83",
      "metadata": {
        "id": "2f9ebd83"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03898fc",
      "metadata": {
        "id": "a03898fc"
      },
      "source": [
        "## Introduction to generative adversarial networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "225a1f95",
      "metadata": {
        "id": "225a1f95"
      },
      "source": [
        "### A schematic GAN implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f295066",
      "metadata": {
        "id": "6f295066"
      },
      "source": [
        "### A bag of tricks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15c08c9f",
      "metadata": {
        "id": "15c08c9f"
      },
      "source": [
        "### Getting our hands on the CelebA dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa811cf7",
      "metadata": {
        "id": "aa811cf7"
      },
      "source": [
        "**Getting the CelebA data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0176e30",
      "metadata": {
        "id": "e0176e30"
      },
      "outputs": [],
      "source": [
        "!mkdir celeba_gan\n",
        "!gdown --id 1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684 -O celeba_gan/data.zip\n",
        "!unzip -qq celeba_gan/data.zip -d celeba_gan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bdfd2dc",
      "metadata": {
        "id": "0bdfd2dc"
      },
      "source": [
        "**Creating a dataset from a directory of images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67fea3a7",
      "metadata": {
        "id": "67fea3a7"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "dataset = keras.utils.image_dataset_from_directory(\n",
        "    \"celeba_gan\",\n",
        "    label_mode=None,\n",
        "    image_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    smart_resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b24f2aaf",
      "metadata": {
        "id": "b24f2aaf"
      },
      "source": [
        "**Rescaling the images**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb64e797",
      "metadata": {
        "id": "fb64e797"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(lambda x: x / 255.)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6a92007",
      "metadata": {
        "id": "e6a92007"
      },
      "source": [
        "**Displaying the first image**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58f13216",
      "metadata": {
        "id": "58f13216"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for x in dataset:\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow((x.numpy() * 255).astype(\"int32\")[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78e9f80",
      "metadata": {
        "id": "f78e9f80"
      },
      "source": [
        "### The discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16893491",
      "metadata": {
        "id": "16893491"
      },
      "source": [
        "**The GAN discriminator network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40b52eee",
      "metadata": {
        "id": "40b52eee"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "discriminator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(64, 64, 3)),\n",
        "        layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Flatten(),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"discriminator\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ae16f4e",
      "metadata": {
        "id": "7ae16f4e"
      },
      "outputs": [],
      "source": [
        "discriminator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0440a90c",
      "metadata": {
        "id": "0440a90c"
      },
      "source": [
        "### The generator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a6d882a",
      "metadata": {
        "id": "5a6d882a"
      },
      "source": [
        "**GAN generator network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1e0b56f",
      "metadata": {
        "id": "b1e0b56f"
      },
      "outputs": [],
      "source": [
        "latent_dim = 128\n",
        "\n",
        "generator = keras.Sequential(\n",
        "    [\n",
        "        keras.Input(shape=(latent_dim,)),\n",
        "        layers.Dense(8 * 8 * 128),\n",
        "        layers.Reshape((8, 8, 128)),\n",
        "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n",
        "        layers.LeakyReLU(alpha=0.2),\n",
        "        layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n",
        "    ],\n",
        "    name=\"generator\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f10f4c",
      "metadata": {
        "id": "d6f10f4c"
      },
      "outputs": [],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "020b1bea",
      "metadata": {
        "id": "020b1bea"
      },
      "source": [
        "### The adversarial network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60ee1f3",
      "metadata": {
        "id": "c60ee1f3"
      },
      "source": [
        "**The GAN `Model`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bb3beb",
      "metadata": {
        "id": "c1bb3beb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "class GAN(keras.Model):\n",
        "    def __init__(self, discriminator, generator, latent_dim):\n",
        "        super().__init__()\n",
        "        self.discriminator = discriminator\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n",
        "        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
        "        super(GAN, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.d_loss_metric, self.g_loss_metric]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "        random_latent_vectors = tf.random.normal(\n",
        "            shape=(batch_size, self.latent_dim))\n",
        "        generated_images = self.generator(random_latent_vectors)\n",
        "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
        "        labels = tf.concat(\n",
        "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))],\n",
        "            axis=0\n",
        "        )\n",
        "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(combined_images)\n",
        "            d_loss = self.loss_fn(labels, predictions)\n",
        "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients(\n",
        "            zip(grads, self.discriminator.trainable_weights)\n",
        "        )\n",
        "\n",
        "        random_latent_vectors = tf.random.normal(\n",
        "            shape=(batch_size, self.latent_dim))\n",
        "\n",
        "        misleading_labels = tf.zeros((batch_size, 1))\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = self.discriminator(\n",
        "                self.generator(random_latent_vectors))\n",
        "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
        "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients(\n",
        "            zip(grads, self.generator.trainable_weights))\n",
        "\n",
        "        self.d_loss_metric.update_state(d_loss)\n",
        "        self.g_loss_metric.update_state(g_loss)\n",
        "        return {\"d_loss\": self.d_loss_metric.result(),\n",
        "                \"g_loss\": self.g_loss_metric.result()}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba2bbe15",
      "metadata": {
        "id": "ba2bbe15"
      },
      "source": [
        "**A callback that samples generated images during training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f70b7e6b",
      "metadata": {
        "id": "f70b7e6b"
      },
      "outputs": [],
      "source": [
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, num_img=3, latent_dim=128):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images *= 255\n",
        "        generated_images.numpy()\n",
        "        for i in range(self.num_img):\n",
        "            img = keras.utils.array_to_img(generated_images[i])\n",
        "            img.save(f\"generated_img_{epoch:03d}_{i}.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4155aef3",
      "metadata": {
        "id": "4155aef3"
      },
      "source": [
        "**Compiling and training the GAN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ef3bb96",
      "metadata": {
        "id": "8ef3bb96"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "\n",
        "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
        "gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
        "    loss_fn=keras.losses.BinaryCrossentropy(),\n",
        ")\n",
        "\n",
        "gan.fit(\n",
        "    dataset, epochs=epochs, callbacks=[GANMonitor(num_img=10, latent_dim=latent_dim)]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dfe7bb9",
      "metadata": {
        "id": "0dfe7bb9"
      },
      "source": [
        "### Wrapping up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac02ac32",
      "metadata": {
        "id": "ac02ac32"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680e8700",
      "metadata": {
        "id": "680e8700"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb537604",
      "metadata": {
        "id": "eb537604"
      },
      "source": [
        "# Best practices for the real world"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41665cb5",
      "metadata": {
        "id": "41665cb5"
      },
      "source": [
        "## Getting the most out of your models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8867965",
      "metadata": {
        "id": "b8867965"
      },
      "source": [
        "### Hyperparameter optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17706864",
      "metadata": {
        "id": "17706864"
      },
      "source": [
        "#### Using KerasTuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "492ff285",
      "metadata": {
        "id": "492ff285"
      },
      "outputs": [],
      "source": [
        "!pip install keras-tuner -q"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84c06d51",
      "metadata": {
        "id": "84c06d51"
      },
      "source": [
        "**A KerasTuner model-building function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad1ea033",
      "metadata": {
        "id": "ad1ea033"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def build_model(hp):\n",
        "    units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "    model = keras.Sequential([\n",
        "        layers.Dense(units, activation=\"relu\"),\n",
        "        layers.Dense(10, activation=\"softmax\")\n",
        "    ])\n",
        "    optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\"accuracy\"])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52a0530a",
      "metadata": {
        "id": "52a0530a"
      },
      "source": [
        "**A KerasTuner `HyperModel`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6b40943",
      "metadata": {
        "id": "c6b40943"
      },
      "outputs": [],
      "source": [
        "import kerastuner as kt\n",
        "\n",
        "class SimpleMLP(kt.HyperModel):\n",
        "    def __init__(self, num_classes):\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def build(self, hp):\n",
        "        units = hp.Int(name=\"units\", min_value=16, max_value=64, step=16)\n",
        "        model = keras.Sequential([\n",
        "            layers.Dense(units, activation=\"relu\"),\n",
        "            layers.Dense(self.num_classes, activation=\"softmax\")\n",
        "        ])\n",
        "        optimizer = hp.Choice(name=\"optimizer\", values=[\"rmsprop\", \"adam\"])\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss=\"sparse_categorical_crossentropy\",\n",
        "            metrics=[\"accuracy\"])\n",
        "        return model\n",
        "\n",
        "hypermodel = SimpleMLP(num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc4ecccb",
      "metadata": {
        "id": "dc4ecccb"
      },
      "outputs": [],
      "source": [
        "tuner = kt.BayesianOptimization(\n",
        "    build_model,\n",
        "    objective=\"val_accuracy\",\n",
        "    max_trials=100,\n",
        "    executions_per_trial=2,\n",
        "    directory=\"mnist_kt_test\",\n",
        "    overwrite=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd7e000",
      "metadata": {
        "id": "0bd7e000"
      },
      "outputs": [],
      "source": [
        "tuner.search_space_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d9d86cc",
      "metadata": {
        "id": "4d9d86cc"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "x_test = x_test.reshape((-1, 28 * 28)).astype(\"float32\") / 255\n",
        "x_train_full = x_train[:]\n",
        "y_train_full = y_train[:]\n",
        "num_val_samples = 10000\n",
        "x_train, x_val = x_train[:-num_val_samples], x_train[-num_val_samples:]\n",
        "y_train, y_val = y_train[:-num_val_samples], y_train[-num_val_samples:]\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5),\n",
        "]\n",
        "tuner.search(\n",
        "    x_train, y_train,\n",
        "    batch_size=128,\n",
        "    epochs=100,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=callbacks,\n",
        "    verbose=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df0c8d50",
      "metadata": {
        "id": "df0c8d50"
      },
      "source": [
        "**Querying the best hyperparameter configurations**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe1c5909",
      "metadata": {
        "id": "fe1c5909"
      },
      "outputs": [],
      "source": [
        "top_n = 4\n",
        "best_hps = tuner.get_best_hyperparameters(top_n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8626ad6",
      "metadata": {
        "id": "a8626ad6"
      },
      "outputs": [],
      "source": [
        "def get_best_epoch(hp):\n",
        "    model = build_model(hp)\n",
        "    callbacks=[\n",
        "        keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_loss\", mode=\"min\", patience=10)\n",
        "    ]\n",
        "    history = model.fit(\n",
        "        x_train, y_train,\n",
        "        validation_data=(x_val, y_val),\n",
        "        epochs=100,\n",
        "        batch_size=128,\n",
        "        callbacks=callbacks)\n",
        "    val_loss_per_epoch = history.history[\"val_loss\"]\n",
        "    best_epoch = val_loss_per_epoch.index(min(val_loss_per_epoch)) + 1\n",
        "    print(f\"Best epoch: {best_epoch}\")\n",
        "    return best_epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f34876ae",
      "metadata": {
        "id": "f34876ae"
      },
      "outputs": [],
      "source": [
        "def get_best_trained_model(hp):\n",
        "    best_epoch = get_best_epoch(hp)\n",
        "    model = build_model(hp)\n",
        "    model.fit(\n",
        "        x_train_full, y_train_full,\n",
        "        batch_size=128, epochs=int(best_epoch * 1.2))\n",
        "    return model\n",
        "\n",
        "best_models = []\n",
        "for hp in best_hps:\n",
        "    model = get_best_trained_model(hp)\n",
        "    model.evaluate(x_test, y_test)\n",
        "    best_models.append(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e63f27",
      "metadata": {
        "id": "63e63f27"
      },
      "outputs": [],
      "source": [
        "best_models = tuner.get_best_models(top_n)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72637795",
      "metadata": {
        "id": "72637795"
      },
      "source": [
        "#### The art of crafting the right search space"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7f4c61e",
      "metadata": {
        "id": "f7f4c61e"
      },
      "source": [
        "#### The future of hyperparameter tuning: automated machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4e097b5",
      "metadata": {
        "id": "e4e097b5"
      },
      "source": [
        "### Model ensembling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52387b5c",
      "metadata": {
        "id": "52387b5c"
      },
      "source": [
        "## Scaling-up model training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75dda6d6",
      "metadata": {
        "id": "75dda6d6"
      },
      "source": [
        "### Speeding up training on GPU with mixed precision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4b79745",
      "metadata": {
        "id": "c4b79745"
      },
      "source": [
        "#### Understanding floating-point precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d65a857",
      "metadata": {
        "id": "9d65a857"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "np_array = np.zeros((2, 2))\n",
        "tf_tensor = tf.convert_to_tensor(np_array)\n",
        "tf_tensor.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eff0164c",
      "metadata": {
        "id": "eff0164c"
      },
      "outputs": [],
      "source": [
        "np_array = np.zeros((2, 2))\n",
        "tf_tensor = tf.convert_to_tensor(np_array, dtype=\"float32\")\n",
        "tf_tensor.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00a03416",
      "metadata": {
        "id": "00a03416"
      },
      "source": [
        "#### Mixed-precision training in practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c6437dc",
      "metadata": {
        "id": "9c6437dc"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e775e475",
      "metadata": {
        "id": "e775e475"
      },
      "source": [
        "### Multi-GPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9686566b",
      "metadata": {
        "id": "9686566b"
      },
      "source": [
        "#### Getting your hands on two or more GPUs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ff60746",
      "metadata": {
        "id": "1ff60746"
      },
      "source": [
        "#### Single-host, multi-device synchronous training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc723237",
      "metadata": {
        "id": "fc723237"
      },
      "source": [
        "### TPU training"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd202bc",
      "metadata": {
        "id": "dfd202bc"
      },
      "source": [
        "#### Using a TPU via Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5c82d6c",
      "metadata": {
        "id": "c5c82d6c"
      },
      "source": [
        "#### Leveraging step fusing to improve TPU utilization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f426bfe",
      "metadata": {
        "id": "8f426bfe"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6be0545",
      "metadata": {
        "id": "f6be0545"
      },
      "source": [
        "This is a companion notebook for the book [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff). For readability, it only contains runnable code blocks and section titles, and omits everything else in the book: text paragraphs, figures, and pseudocode.\n",
        "\n",
        "**If you want to be able to follow what's going on, I recommend reading the notebook side by side with your copy of the book.**\n",
        "\n",
        "This notebook was generated for TensorFlow 2.6."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c42083c4",
      "metadata": {
        "id": "c42083c4"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61edc35",
      "metadata": {
        "id": "c61edc35"
      },
      "source": [
        "## Key concepts in review"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2edf9f24",
      "metadata": {
        "id": "2edf9f24"
      },
      "source": [
        "### Various approaches to AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976a6910",
      "metadata": {
        "id": "976a6910"
      },
      "source": [
        "### What makes deep learning special within the field of machine learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91a69f6b",
      "metadata": {
        "id": "91a69f6b"
      },
      "source": [
        "### How to think about deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e43ce4a",
      "metadata": {
        "id": "5e43ce4a"
      },
      "source": [
        "### Key enabling technologies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c9db5f",
      "metadata": {
        "id": "48c9db5f"
      },
      "source": [
        "### The universal machine-learning workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc42256a",
      "metadata": {
        "id": "dc42256a"
      },
      "source": [
        "### Key network architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80562345",
      "metadata": {
        "id": "80562345"
      },
      "source": [
        "#### Densely connected networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fccb6bdd",
      "metadata": {
        "id": "fccb6bdd"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d878333",
      "metadata": {
        "id": "9d878333"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78f25998",
      "metadata": {
        "id": "78f25998"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "100fa821",
      "metadata": {
        "id": "100fa821"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_input_features,))\n",
        "x = layers.Dense(32, activation=\"relu\")(inputs)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs layers.Dense(num_values)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"mse\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c5dc91d",
      "metadata": {
        "id": "7c5dc91d"
      },
      "source": [
        "#### Convnets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad32bfd7",
      "metadata": {
        "id": "ad32bfd7"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(height, width, channels))\n",
        "x = layers.SeparableConv2D(32, 3, activation=\"relu\")(inputs)\n",
        "x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
        "x = layers.SeparableConv2D(128, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(2)(x)\n",
        "x = layers.SeparableConv2D(64, 3, activation=\"relu\")(x)\n",
        "x = layers.SeparableConv2D(128, 3, activation=\"relu\")(x)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c22087",
      "metadata": {
        "id": "94c22087"
      },
      "source": [
        "#### RNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a23459f",
      "metadata": {
        "id": "6a23459f"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_timesteps, num_features))\n",
        "x = layers.LSTM(32)(inputs)\n",
        "outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9c822c",
      "metadata": {
        "id": "4b9c822c"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(num_timesteps, num_features))\n",
        "x = layers.LSTM(32, return_sequences=True)(inputs)\n",
        "x = layers.LSTM(32, return_sequences=True)(x)\n",
        "x = layers.LSTM(32)(x)\n",
        "outputs = layers.Dense(num_classes, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae75f37",
      "metadata": {
        "id": "dae75f37"
      },
      "source": [
        "#### Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1fda8cf",
      "metadata": {
        "id": "c1fda8cf"
      },
      "outputs": [],
      "source": [
        "encoder_inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "transformer.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a14fdb81",
      "metadata": {
        "id": "a14fdb81"
      },
      "outputs": [],
      "source": [
        "inputs = keras.Input(shape=(sequence_length,), dtype=\"int64\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
        "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79cb64bd",
      "metadata": {
        "id": "79cb64bd"
      },
      "source": [
        "### The space of possibilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5bc2291",
      "metadata": {
        "id": "a5bc2291"
      },
      "source": [
        "## The limitations of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89af313f",
      "metadata": {
        "id": "89af313f"
      },
      "source": [
        "### The risk of anthropomorphizing machine-learning models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "650434bd",
      "metadata": {
        "id": "650434bd"
      },
      "source": [
        "### Automatons vs. intelligent agents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa2c145",
      "metadata": {
        "id": "0aa2c145"
      },
      "source": [
        "### Local generalization vs. extreme generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68651fb",
      "metadata": {
        "id": "d68651fb"
      },
      "source": [
        "### The purpose of intelligence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21da1eea",
      "metadata": {
        "id": "21da1eea"
      },
      "source": [
        "### Climbing the spectrum of generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e129f35d",
      "metadata": {
        "id": "e129f35d"
      },
      "source": [
        "## Setting the course toward greater generality in AI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed02d08c",
      "metadata": {
        "id": "ed02d08c"
      },
      "source": [
        "### On the importance of setting the right objective: The shortcut rule"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f34416b",
      "metadata": {
        "id": "5f34416b"
      },
      "source": [
        "### A new target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc056275",
      "metadata": {
        "id": "cc056275"
      },
      "source": [
        "## Implementing intelligence: The missing ingredients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "879a7ab3",
      "metadata": {
        "id": "879a7ab3"
      },
      "source": [
        "### Intelligence as sensitivity to abstract analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd77a8fd",
      "metadata": {
        "id": "bd77a8fd"
      },
      "source": [
        "### The two poles of abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c2b2b41",
      "metadata": {
        "id": "7c2b2b41"
      },
      "source": [
        "#### Value-centric analogy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec4deb13",
      "metadata": {
        "id": "ec4deb13"
      },
      "source": [
        "#### Program-centric analogy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87065f5",
      "metadata": {
        "id": "b87065f5"
      },
      "source": [
        "#### Cognition as a combination of both kinds of abstraction"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a0172f2",
      "metadata": {
        "id": "3a0172f2"
      },
      "source": [
        "### The missing half of the picture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b35c290",
      "metadata": {
        "id": "9b35c290"
      },
      "source": [
        "## The future of deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3eb2e5a",
      "metadata": {
        "id": "a3eb2e5a"
      },
      "source": [
        "### Models as programs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18dc97a",
      "metadata": {
        "id": "a18dc97a"
      },
      "source": [
        "### Blending together deep learning and program synthesis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c03074",
      "metadata": {
        "id": "07c03074"
      },
      "source": [
        "#### Integrating deep-learning modules and algorithmic modules into hybrid systems"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71a5d710",
      "metadata": {
        "id": "71a5d710"
      },
      "source": [
        "#### Using deep learning to guide program search"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb73e3a7",
      "metadata": {
        "id": "fb73e3a7"
      },
      "source": [
        "### Lifelong learning and modular subroutine reuse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26094313",
      "metadata": {
        "id": "26094313"
      },
      "source": [
        "### The long-term vision"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f9f1a8b",
      "metadata": {
        "id": "7f9f1a8b"
      },
      "source": [
        "## Staying up to date in a fast-moving field"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba0066b2",
      "metadata": {
        "id": "ba0066b2"
      },
      "source": [
        "### Practice on real-world problems using Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "881f7898",
      "metadata": {
        "id": "881f7898"
      },
      "source": [
        "### Read about the latest developments on arXiv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54a5d02b",
      "metadata": {
        "id": "54a5d02b"
      },
      "source": [
        "### Explore the Keras ecosystem"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecf8860c",
      "metadata": {
        "id": "ecf8860c"
      },
      "source": [
        "## Final words"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}